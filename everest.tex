\documentclass[]{emulateapj}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath,mathtools}
\usepackage{epsfig}
\usepackage[FIGTOPCAP]{subfigure}
\usepackage{afterpage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{relsize}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{morefloats}
\usepackage{wasysym}

\newcommand{\noop}[1]{}
\newcommand{\note}[1]{{\color{red} #1}}
\newcommand{\cn}{\note{(citation needed)\ }}
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\newcommand{\mearth}{\unit{M_\oplus}}
\newcommand{\rearth}{\unit{R_\oplus}}
\newcommand{\msun}{\unit{M_\odot}}
\newcommand{\lsun}{\unit{L_\odot}}
\newcommand{\mstar}{\unit{M_\star}}
\newcommand{\rj}{\ensuremath{R_\mathrm{J}}}
\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\bavg}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\newcommand{\Kp}{\ensuremath{K_\mathrm{p}}}
\DeclareMathOperator*{\argmin}{arg\,min}

\shorttitle{EVEREST 2.0}
\shortauthors{Luger et al. 2016}

\begin{document}

\title{An update to the EVEREST K2 pipeline:\\ Short cadence, saturated stars, and Kepler-like photometry down to $\Kp = 15$}
\author{Rodrigo Luger\altaffilmark{1,2}, Eric Agol\altaffilmark{1,2}, Ethan Kruse\altaffilmark{1}, \\
Daniel Foreman-Mackey\altaffilmark{1,3}, Nicholas Saunders\altaffilmark{1}, Rory Barnes\altaffilmark{1,2}}
\altaffiltext{1}{Astronomy Department, University of Washington, Box 351580, Seattle, WA 98195, USA; \href{mailto:rodluger@uw.edu}{rodluger@uw.edu}}
\altaffiltext{2}{Virtual Planetary Laboratory, Seattle, WA 98195, USA}
\altaffiltext{3}{Sagan Fellow}

\begin{abstract}
We present an update to the \texttt{EVEREST} pipeline.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Introduction.

\section{The PLD Model}
\label{sec:model}
Here we describe the mathematical formulation of the \texttt{EVEREST}
pixel level decorrelation (PLD) model. In PLD, products of the fractional fluxes in each pixel
of the target aperture are used as regressors in a linear model:
%
\begin{align}
\label{eq:pldmodel}
\mathbf{m} = &\sum\limits_{i}                                 a_i     \frac{\mathbf{p}_{i}}                             { \sum\limits_{n}\mathbf{p}_{n}} +     \nonumber\\
             &\sum\limits_{i} \sum\limits_{j}                 b_{ij}  \frac{\mathbf{p}_{i}\mathbf{p}_{j}}               {(\sum\limits_{n}\mathbf{p}_{n})^2} +  \nonumber\\
             &\sum\limits_{i} \sum\limits_{j} \sum\limits_{k} c_{ijk} \frac{\mathbf{p}_{i}\mathbf{p}_{j}\mathbf{p}_{k}} {(\sum\limits_{n}\mathbf{p}_{n})^3}.
\end{align}
%
In the expression above, $\mathbf{m}$ is the model and $\mathbf{p}_{i}$ is the flux in
the $i^{th}$ pixel; both are vector quantities defined at an array of times $\mathbf{t}$.
The $a_i$, $b_{ij}$, and $c_{ijk}$ are the linear weights of the model, which we seek
to obtain below. For a detailed discussion of the theory behind PLD, see 
\cite{Deming15} and Paper I. Below we simply discuss its mathematical implementation.

\subsection{Ridge Regression}
\label{sec:ridge}
Given a timeseries $\mathbf{y}$ with $N_{dat}$ data points, we wish to find the linear
combination of $N_{reg}$ regressors that best fits the instrumental component of $\mathbf{y}$.
Expressed in vector form, our linear model is thus
%
\begin{align}
\label{eq:xdotw}
\mathbf{m} = \mathbf{X} \cdot \mathbf{w},
\end{align}
%
where $\mathbf{X}$ is the ($N_{dat} \times N_{reg}$) design matrix constructed from the set
of regressors (the fractional pixel fluxes in Equation~\ref{eq:pldmodel}) and $\mathbf{w}$ is 
the ($N_{reg} \times 1$) vector of weights (the set
$\{a_i, b_{ij}, c_{ijk}\}$). If $\mathbf{w}$ is known, the de-trended light curve is simply
%
\begin{align}
\label{eq:detrended}
\mathbf{y}' = \mathbf{y} - \mathbf{m}.
\end{align}
%
In Paper I, we obtained $\mathbf{w}$ by maximizing the likelihood function
%
\begin{align}
\label{eq:like0}
\log\mathcal{L}_0 =  &-\frac{1}{2} \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right)^\top
                     \cdot
                     \mathbf{K^{-1}}
                     \cdot
                     \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right) \nonumber\\  
                     %
                     &-\frac{1}{2} \log\left|\mathbf{K}\right| 
                     -\frac{N_{dat}}{2}\log 2\pi,
\end{align}
%
where $\mathbf{K}$ is the ($N_{dat} \times N_{dat}$) covariance matrix of the data and
$\mathbf{y}$ is the ($N_{dat} \times 1$) SAP flux. Since the number of third order PLD regressors can
be quite large (on the order of several thousand for a typical star, which is larger than the number
of data points), the problem is ill-posed, meaning that a unique solution does not exist and 
maximizing $\log\mathcal{L}_0$ is likely to lead to overfitting. We thus constructed $\mathbf{X}$
from the (smaller) set of $N_{pca}$ principal components of the PLD regressors. We chose $N_{pca}$
by performing cross-validation, which aims to maximize the predictive power of the model while
minimizing overfitting.

However, while principal component analysis (PCA) yields a set of components that captures 
the most variance among the PLD vectors, there is no guarantee that the principal components
are the ideal regressors in the PLD problem. Dimensionality reduction techniques such as PCA
inevitably lead to information loss, and so it is worthwhile to consider alternative
regression methods to fully exploit the potential of PLD.

A common regression method for ill-posed problems is regularization, in which a prior is imposed
on the values of the weights $\mathbf{w}$. Since overfitting occurs when $\mathbf{w}$ becomes very
large, regularization recasts the problem by adding a penalty term to the likelihood that 
increases with increasing $|\mathbf{w}|$. While many forms of regularization exist, we focus on
ridge (L2) regression, since it has an analytic solution. Ridge regression involves placing a
Gaussian prior on each of the weights $\mathbf{w}$, so that the posterior likelihood function becomes
%
\begin{align}
\label{eq:like}
\log\mathcal{L} =  \log\mathcal{L}_0
                   -\frac{1}{2}
                   \mathbf{w}^\top \cdot \mathbf{\Lambda}^{-1} \cdot \mathbf{w}
                   -\frac{1}{2} \log\left|\mathbf{\Lambda}\right|,
\end{align}
%
where $\mathbf{\Lambda}$ is the ($N_{reg} \times N_{reg}$) diagonal regularization matrix,
%
\begin{align}
\label{eq:Lambda}
\Lambda_{m,n} = \lambda_{n}^2\delta_{mn}.
\end{align}
%
Each element $\lambda_n^2$ in $\mathbf{\Lambda}$ is the variance of the 
zero-mean Gaussian prior on the weight of the corresponding column of the design matrix, 
$\mathbf{X}_{*,n}$. Provided we choose the $\lambda_{n}$ correctly, this model should 
have a higher predictive power than the PCA model adopted in Paper I.

Given this formulation, our task is to find the weights $\mathbf{\hat{w}}$ that maximize 
the posterior probability 
$\mathcal{L}$. Differentiating 
Equation~(\ref{eq:like}) with respect to $\mathbf{w}$, we get
%
\begin{align}
\label{eq:gradlike}
\frac{\mathrm{d}\mathbf{\log\mathcal{L}}}{\mathrm{d}\mathbf{w}} &= 
%
\mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y} \nonumber\\
%
&- \left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right) \cdot \mathbf{w}.
\end{align}
%
By setting this expression equal to zero, we obtain the maximum \emph{a posteriori} prediction 
for the weights,
%
\begin{align}
\label{eq:what}
\mathbf{\hat{w}} = 
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}
\end{align}
with corresponding model
%
\begin{align}
\label{eq:model_slow}
\mathbf{m} = 
%
\mathbf{X} \cdot
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}.
\end{align}

\subsection{Cross-validation}
\label{sec:crossval}
Similarly to Paper I, we solve for $\mathbf{\Lambda}$ by cross-validation. For each value
of $\mathbf{\Lambda}$, the model is trained on one part of the light curve (the training set)
and used to de-trend the other part of the light curve (the validation set). The value of 
$\mathbf{\Lambda}$ that results in the minimum scatter in the validation set is then chosen for
the final de-trending step. 

In principle, each of the $\lambda_n$ in $\mathbf{\Lambda}$ could take on a different value, 
but solving for each one requires minimizing an $N_{reg}$-dimensional function and is not computationally tractable.
Instead, we simplify the problem by requiring that all regressors of the same order have the
same regularization parameter $\lambda$. Provided we write the third order design matrix in the form
%
\begin{align}
\label{eq:design}
\mathbf{X} = 
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right),
\end{align}
%
where $\mathbf{X_n}$ is the matrix of $n^\mathrm{th}$ order regressors, we may
express the regularization matrix as
%
\begin{align}
\label{eq:Lambda_block}
\mathbf{\Lambda} = 
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}      &                       & \\
  &                       \mathbf{\Lambda_2}      & \\
  &                       &                       \mathbf{\Lambda_3} \\
\end{array}
\right)
\end{align}
%
where $\mathbf{\Lambda_n} = \lambda_{n}^2\mathbf{I}$ is the 
$n^\mathrm{th}$ order regularization matrix and $\lambda_{n}^2$ is the variance
of the prior on the $n^\mathrm{th}$ order regressors. 

A typical $K2$ star with 30 aperture pixels has $N_{reg} \sim\ $5,000 regressors and
$N_{dat} \sim\ $500 data points in each cross-validation light curve segment 
(see \S\ref{sec:impl_crossval}). Evaluating the matrix inverse in Equation~(\ref{eq:model_slow})
is thus computationally expensive, and becomes prohibitive during cross-validation,
since this must be done for every set of $\lambda_{n}$'s. Fortunately, we can reduce 
the number of calculations with some linear algebra. First, we apply the Woodbury matrix 
identity to Equation~(\ref{eq:model_slow}), obtaining
%
\begin{align}
\label{eq:model_woodbury}
\mathbf{m} =      \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top
                  \cdot
                  \left(
                  \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
Next, we note that
%
\begin{align}
\label{eq:separable1}
\mathbf{X} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{\Lambda} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X}^\top &= 
%
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right)
%
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}\hspace*{-8pt}      &                                     & \\[2pt]
  &                                     \mathbf{\Lambda_2}\hspace*{-8pt}      & \\[2pt]
  &                                     &                                     \mathbf{\Lambda_3} \\
\end{array}
\right)
%
\left(
\begin{array}{c}
  \mathbf{X_1^\top} \\[2pt]
  \mathbf{X_2^\top} \\[2pt]
  \mathbf{X_3^\top} \\
\end{array}
\right) \nonumber\\[5pt]
%
&= \lambda_1^2 \mathbf{X_1} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_1} +
   \lambda_2^2 \mathbf{X_2} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_2} +
   \lambda_3^2 \mathbf{X_3} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_3} \nonumber\\[5pt]
%
&= \sum_n \lambda_n^2 \mathbf{X^2_n},
\end{align}
%
where we have defined
%
\begin{align}
\label{eq:x2}
\mathbf{X^2_n} \equiv \mathbf{X_n} \cdot \mathbf{X^\top_n}.
\end{align}
%
We may thus re-write our maximum \emph{a posteriori} model as
%
\begin{align}
\label{eq:model}
\mathbf{m} = \sum_n \lambda_n^2 \mathbf{X^2_n}
                  \cdot
                  \left(
                  \sum_n \lambda_n^2 \mathbf{X^2_n} + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
The matrix that we must invert in Equation~(\ref{eq:model}) has dimensions ($N_{dat} \times N_{dat}$),
while that in Equation~(\ref{eq:model_slow}) is ($N_{reg} \times N_{reg}$). Since
$N_{reg} \sim 10N_{dat}$, casting the model in this form can greatly speed up the
computation. In practice, we pre-compute the three matrices $\mathbf{X^2_n}$ at the beginning
of the cross-validation step, so the only time-consuming operation in Equation~(\ref{eq:model})
is the inversion.

\subsection{Neighboring Stars}
\label{sec:neighboring}
The principal difference between PLD and other methods used to correct $K2$ light curves
\citep[e.g.,][]{Vanderburg14,ForemanMackey15,Aigrain16} is in the origin of the information
used in the de-trending. PLD uses \emph{local} information, present in a given star's pixels,
to de-trend that star's light curve; alternative methods almost exclusively use \emph{global} information,
derived from the behaviour of many stars across the detector, to accomplish the same thing. 
It is perhaps surprising that PLD works so well, given that the signal-to-noise ratio of the regressors
of the latter methods is \emph{necessarily} higher, since they are derived from hundreds to
thousands of stars and thus contain information about the spacecraft motion at high fidelity.
The fact that PLD outperforms other pipelines --- and in particular its ability to correct
data collected during thruster firings --- suggests that local modes contribute significantly
to the instrumental noise in $K2$ light curves. Local noise is due to intra- and inter-pixel
variability and local pixel response function (PRF) behavior and is specific to the properties of 
the detector at the particular location of the target, while global noise is due solely to 
the motion of the spacecraft and is shared among all targets. Traditional methods can only
correct the latter (but do so extremely well). PLD excels at correcting the former. While
PLD certainly removes the noise introduced by the spacecraft motion (i.e., global noise), its
regressors are noisy, particularly for dimmer targets. This suggests that a combination of
both approaches that explicitly de-trends both local and global noise could substantially improve PLD,
especially for faint stars.

A simple way of implementing this would be to include the two \emph{Kepler} motion vectors (the
\texttt{POS\_CORR1} and \texttt{POS\_CORR2} fields in the target pixel files), as well as
higher order products of these, in the regression, assigning each their own L2 priors ($\mathbf{\Lambda}$).
While this approach works, in practice we obtain higher de-trending power by instead including the PLD
vectors of bright neighboring targets in the regression; we dub this method \texttt{nPLD}, for \emph{n}eighboring
\emph{PLD}. Our third-order design matrix (Equation~\ref{eq:design}) is now
%
\begin{align}
\label{eq:design_nPLD}
\mathbf{X} = 
\left(
\begin{array}{cccccc}
  \mathbf{X_1} & \mathbf{X_1'} & \mathbf{X_2} & \mathbf{X_2'} & \mathbf{X_3} & \mathbf{X_3'}
\end{array}
\right),
\end{align}
%
where $\mathbf{X_n'}$ is the design matrix constructed from the $n^{th}$ order PLD vectors of all the 
neighboring targets. For computational speed, we still solve for a single prior amplitude $\lambda_n$ 
for each PLD order, but in principle one could assign different priors to the neighboring vectors. We
discuss the implementation of \texttt{nPLD} in \S\ref{sec:implementation}.

\section{Implementation}
\label{sec:implementation}

\subsection{Light curves}
\label{sec:impl_lightcurves}
As in Paper I, we downloaded all stars in the $K2$ EPIC catalog with long and/or short cadence
target pixel files and adopted aperture \#15
from the \texttt{K2SFF} catalog \citep{Vanderburg14,VanderburgJohnson14}. We masked all
cadences with \texttt{QUALITY} flags 1-9, 11-14, and 16-17, though we still compute the
model prediction on them. For campaigns 0-2, we remove the background signal as 
described in Paper I; for more recent campaigns, the background is removed by the \emph{Kepler}
team.

Next, we perform iterative sigma clipping to identify and mask outliers at 5$\sigma$.
During each iteration, we compute the linear, unregularized PLD model and smooth it with
a Savitsky-Golay filter \citep{SavitskyGolay64}, then identify outliers based on a 
median absolute deviation (MAD) cut. We implement this outlier-clipping step at the
beginning of each cross-validation step (\S\ref{sec:impl_crossval}), each time computing
the model with a higher PLD order, to progressively refine the outlier mask.

\subsection{GP Optimization}
\label{sec:impl_gp}
In order to compute the covariance matrix $\mathbf{K}$ for each target, we use a Gaussian
process (GP), as we did in Paper I. GP optimization can be costly, especially when
performing model selection over a range of possible kernels and optimizing many
hyperparameters simultaneously. For this reason, in Paper I we cut corners and performed
kernel selection based on fits to the autocorrelation function of the light curve, which
we also used to fix the timescale and/or period of those kernel(s). We then ran a nonlinear
minimizer to optimize the overall amplitude of the GP. In practice, this worked reasonably
well, but often failed for light curves dominated by high frequency stellar variability.
After much experimentation, we decided to forego the kernel selection step in favor of
using a single carefully optimized white kernel plus a Mat\'ern-3/2 kernel:
%
\begin{align}
\mathbf{K}_{ij} = \sigma^2\delta_{ij} + \alpha \left(1 + \sqrt{3(t_i - t_j)^2}\right) e^{-\sqrt{3(t_i - t_j)^2}/\tau},
\end{align}
%
where the hyperparameters $\sigma$, $\alpha$, and $\tau$ are the white noise amplitude,
red noise amplitude, and red noise timescale, respectively, and $t_i$ and $t_j$ correspond
to the timestamps of cadences $i$ and $j$. We initialize the hyperparameters at random
values and run a nonlinear optimizer to solve for the maximum likelihood (Equation~\ref{eq:like}),
keeping the PLD model parameters fixed; we repeat this process several times and retain the 
highest likelihood solution. As with outlier clipping, we progressively optimize the GP
at each of the three cross-validation steps, so that each time we train the GP on a 
light curve that is increasingly dominated by stellar variability (as opposed to instrumental
systematics).

In principle, the quasi-periodic kernels used in Paper I should be better suited to 
handling variable stars, but in practice we find that a properly optimized Mat\'ern-3/2 
kernel is flexible enough to fully capture the variability and prevent PLD overfitting.
We discuss this further in \S\ref{sec:limitations}.

\subsection{Breakpoints}
\label{sec:impl_breakpoints}
Because the instrumental noise properties are quite variable over the course of $K2$ 
campaigns, we find a significant improvement in the de-trending power of our ridge
regression model when we subdivide light curves into two or three segments. This is in contrast to
the PCA approach in Paper I, where we did not find it necessary to split the timeseries.
For all campaigns except 4 and 7, we add a single breakpoint in the light curve near the
mid-campaign point, where the spacecraft roll is at a minimum. For campaigns 4 and 7,
we find it necessary to insert two breakpoints. We cross-validate and de-trend each 
light curve segment separately and mend them at the end. In order to mitigate flux
discontinuities at the breakpoints, we train the model in each segment on an additional
100 cadences past the breakpoint to remove potential edge effects and offset the models
in each segment so that they align at the breakpoint.
While this method removes flux discontinuities, it can introduce discontinuities in the
derivative of the flux, showing up as spurious ``kinks'' in the light curve. We 
remove these in a post-processing step (\S\ref{sec:cbvs}).

\subsection{Neighboring stars}
\label{sec:impl_neighboring}
In principle, the larger the number of neighboring PLD vectors we include in the
design matrix, the higher the de-trending power of our model. However, adding 
regressors significantly increases computing time, so we would like to instead select
a small set of high signal-to-noise regressors that capture most of the spacecraft
motion information. Moreover, since we employ a single prior for all $n^{th}$ order
regressors, adding many foreign PLD vectors effectively results in an exchange of
local for global de-trending power (\S\ref{sec:neighboring}) and in practice results
in poorer quality light curves. After much experimenting, we find that the ideal number
of neighboring stars to include in the design matrix is about ten. We therefore de-trend
each $K2$ target with the aid of the PLD vectors of ten randomly selected bright 
($11 \leq \Kp \leq 13$) stars on the same detector module as the target. To minimize
contamination of the target by outliers in its neighbors' fluxes, we linearly
interpolate over all neighbor data with flagged \texttt{QUALITY} bits. Finally, for
computational reasons, we neglect all cross terms of the form $\prod_{i \neq j} p_i p_j$,
where $p_i$ is the flux in the $i^{th}$ pixel,
when computing the neighbors' PLD vectors. Cross terms typically encode information
specific to the sets of pixels from which they are computed and aid in correcting
features such as thruster firing discontinuities \citep{Luger16}. Cross terms from
stars other than the target in question are therefore of little help in the de-trending
and can be safely neglected.

One potential pitfall of \texttt{nPLD} is that if the PLD assumptions break down for
any of the neighboring targets, the PLD regressors may become contaminated with
astrophysical information from that neighbor. This is not in general an issue, since
overfitting would only occur if an astrophysical signal in the target star and in
its neighbor had the same period and the same phase. However, in the (unlikely) case
that PLD fails for the neighboring star and this star happens to be an eclipsing binary 
or a transiting exoplanet host, it is possible that its transit signals could get imprinted onto
the target star's de-trended light curve, resulting in potential false positive planet
detections down the line. The two cases relevant to $K2$ in which PLD could fail in such
a way are for saturated stars and stars with bright contaminant sources in their
apertures \citep{Luger16}. As we show in \S\ref{sec:impl_saturated} below, it is 
straightforward to adapt PLD to work reliably for saturated stars. But while 
\texttt{EVEREST 2.0} is more robust against overfitting of crowded stars 
(\S\ref{sec:crowding}), highly crowded apertures remain an issue for PLD. When de-trending
with \texttt{nPLD}, we therefore select neighboring stars with no other known sources
in their apertures that are bright enough ($\Delta \Kp < 5$) to contaminate the PLD
vectors.

\subsection{Saturated stars}
\label{sec:impl_saturated}
As discussed in Paper I, PLD typically fails for stars with saturated pixels, resulting
in overfitted light curves with artificially low CDPP and suppressed astrophysical 
information (such as transits with significantly shallower depths). This happens because
saturated pixels contain nearly no astrophysical information, as the signal
overflows into adjacent pixels in the same column and is ultimately dumped into the
pixels at the top and bottom of the bleed trails; these tail pixels ultimately contain more 
astrophysical information than the other pixels in the aperture. Since PLD implicitly
assumes that astrophysical information is constant across the aperture, the method
breaks down for these stars, and PLD vectors from pixels in the saturated columns become capable
of fitting out the astrophysical information in the rest of the aperture.

We suggested in Paper I that collapsing saturated columns into single pixels --- by co-adding
the fluxes in each of the pixels and treating the resulting timeseries as a single PLD pixel ---
could negate the effect of saturation, since charge is conserved along the bleed trail. While
this ensures PLD does not overfit, it leads to the loss of some of the information about the 
vertical motion of the stellar PSF across the detector. This leads to significantly poorer
de-trending, and therefore we did not employ this method in the first version of the pipeline.
However, we find that including the PLD vectors of neighboring stars in the design matrix 
(i.e., \texttt{nPLD}) effectively restores the information lost when saturated columns are 
collapsed, leading to high quality de-trended light curves of saturated stars. 

In practice, we collapse all columns containing one or more pixels whose flux 
comes within 10\% of (or exceeds)
the pixel well depth for the corresponding detector channel during more than 2.5\% of the
timeseries. We obtained the well depths from
Table 13 of the Kepler Instrument Handbook.
\footnote{\url{archive.stsci.edu/kepler/manuals/KSCI-19033-001.pdf}} 

As an example, in Figure~\ref{fig:saturated_star} we plot the light curve of EPIC 202063160, a saturated
campaign 0 eclipsing binary. The raw light curve is shown at the top and the light curve
de-trended with \texttt{EVEREST 1.0} is shown at the center. Because three of the columns
in the aperture contain saturated pixels (right panel), \texttt{EVEREST 1.0} almost 
completely fits out the eclipses. With column-collapsed \texttt{nPLD} (bottom), the
eclipse is preserved and the instrumental signal is effectively removed.

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/saturated_star.pdf}
       \caption{EPIC 202063160, a saturated \Kp = 9.2 campaign 0 eclipsing
       binary. Shown is a portion of the raw light curve (top), the light curve ``de-trended''
       with \texttt{EVEREST 1.0} (center), and the light curve de-trended with \texttt{EVEREST 2.0}
       (bottom). The pixel image is shown at the right on a linear scale, with the adopted aperture
       contour indicated in red. The three columns highlighted in red contain saturated
       pixels. Despite a great improvement in the CDPP, \texttt{EVEREST 1.0} leads to severe
       overfitting, causing the eclipses to all but disappear. By collapsing saturated columns,
       \texttt{EVEREST 2.0} correctly de-trends saturated stars without overfitting.}
     \label{fig:saturated_star}
  \end{center}
\end{figure}

\subsection{Short cadence}
\label{sec:impl_shortcad}
We treat short cadence targets in much the same way as long cadence targets, with the exception
that we find it necessary to introduce more breakpoints in the light curves. This is
due primarily to computational reasons (short cadence $K2$ light curves are over $10^5$ cadences
in length; computing Equation~(\ref{eq:model}) for the entire light curve is not feasible).
Moreover, we find that noise on sub-30 minute timescales is only properly removed when
the size of the light curve segments is kept small. In practice, we find that on the order of
30 breakpoints result in the best de-trending. This might raise concerns of overfitting, but
since short cadence light curves contains 30 times more data than long cadence light curves,
and we split the latter into two segments, each of the short cadence segments has about twice
as many cadences as the long cadence ones.

The major downside of such a large number of segments are the discontinuities that could
be introduced at each breakpoint. As before, we overcompute the model into adjacent
segments and match the models at the breakpoints, but some de-trended light curves display
occasional jumps in either the flux or its derivative.

A second issue with short cadence light curves concerns deep transits and eclipses. As
we discussed in Paper I, PLD may attempt to fit out these features if they are not
properly masked, since doing so can result in a very large (but spurious) CDPP improvement.
With long cadence light curves, transit masking can be done by the user by simply re-computing
the model with the appropriate cadences masked, since the transits are sparse and their
presence does not significantly affect the cross-validation step. Moreover, outlier clipping
usually masks most deep transits anyways, so this is hardly ever a problem.
However, that is not the case with short cadence light curves, where transits and eclipses 
span upwards of fifty contiguous cadences. Since these features are so smooth, they are
not flagged as outliers. And since the transit signal is no longer sparse --- as it makes
up a substantial fraction of the light curve segment --- it is far more likely to bias
the cross-validation step. In practice, we find that this leads to substantial 
\emph{under}fitting of short cadence light curves with deep transits. As $\lambda_n$
increases, PLD begins to fit out the transit and the scatter in the validation set
grows, forcing the algorithm to select very low values of $\hat{\lambda}_n$ and resulting
in de-trended light curves that still contain significant instrumental signals.

We therefore explicitly mask all deep transits and eclipses in the short cadence light
curves \emph{before} the cross-validation step. Since only deep transits are likely to
bias the cross-validation, and since the number of short cadence light curves in each
campaign is relatively small, these can easily be identified by inspection.

\subsection{Cross-validation}
\label{sec:impl_crossval}
The principal step in the de-trending process is determining the prior amplitudes $\lambda_n$ in
Equation~(\ref{eq:model}), which we do by cross-validation. Our method is analogous to
that of Paper I, where we performed cross-validation to obtain the optimal number of principal
components to regress on. However, here we seek to optimize a three-dimensional function
$\sigma_\mathrm{v}(\lambda_1, \lambda_2, \lambda_3)$, where $\sigma_\mathrm{v}$ is the scatter 
in the validation set; this is a far more expensive calculation to do.
While we could employ a nonlinear optimization
algorithm (see below), in the interest of computational speed, we perform a simplification. Since we
expect the first order PLD regressors to contain most of the de-trending information,
with each successive PLD order providing a small correction term to the fit, we break
down the minimization problem into three separate one-dimensional problems.
First, we perform cross-validation on the first order PLD model by setting $\lambda_2 = \lambda_3 = 0$
to obtain the optimal value of $\lambda_1$, $\hat{\lambda}_1$. We then perform cross-validation on the second order model
by fixing $\lambda_1$ at this estimate and keeping $\lambda_3 = 0$. Finally, we solve
for $\hat{\lambda}_3$ by fixing the first and second order parameters at their optimum
values:
%
\begin{align}
\hat{\lambda}_1 &= \argmin \sigma_\mathrm{v}(\lambda_1)\bigg|_{\lambda_2 = 0,\               \lambda_3 = 0} \nonumber\\
\hat{\lambda}_2 &= \argmin \sigma_\mathrm{v}(\lambda_2)\bigg|_{\lambda_1 = \hat{\lambda}_1,\ \lambda_3 = 0} \nonumber\\
\hat{\lambda}_3 &= \argmin \sigma_\mathrm{v}(\lambda_3)\bigg|_{\lambda_1 = \hat{\lambda}_1,\ \lambda_2 = \hat{\lambda}_2}
\end{align}
% 
It is important to note that there is no \emph{a priori} reason
that this method should yield the global minimum of $\sigma_\mathrm{v}$; in fact, it very
likely does not. However, we explicitly allow for $\lambda_n = 0$ in our grid search, and thus 
this approximation cannot lead to overfitting, as it will always prefer a lower-order PLD model to one
with higher scatter in the validation set. 

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/pPLD.pdf}
       \caption{6-hr CDPP comparison between de-trending with \texttt{pPLD} and
                de-trending with \texttt{nPLD} for a sample of 2,700 randomly selected campaign 6
                stars. Plotted is the star-by-star difference in the CDPP values for each method,
                normalized to the \texttt{nPLD} CDPP (blue dots); stars with negative values have lower CDPP 
                when de-trended with \texttt{pPLD}. The black line is the median CDPP difference
                in 0.5 magnitude-wide bins. \texttt{pPLD} leads to an average improvement
                in the CDPP of ${\lesssim}1\%$.}
     \label{fig:pPLD}
  \end{center}
\end{figure}

As a proof of concept, we de-trended a sample of 2,700 randomly selected campaign 6 targets
using this approximation (henceforth the ``\texttt{nPLD} solution'') in the cross-validation step. 
We then repeated the de-trending by
solving for the $\hat{\lambda}_n$ using Powell's method, initializing the solver at different
points in the vicinity of the \texttt{nPLD} solution and keeping the solution with the lowest
average CDPP for each target; we dub this method \texttt{pPLD}. In Figure~\ref{fig:pPLD} we plot
the star-by-star CDPP difference between the two models, 
$\mathrm{(CDPP_{pPLD} - CDPP_{nPLD})/CDPP_{nPLD}}$. While for some stars the CDPP improves
substantially with \texttt{pPLD}, cross-validating with Powell's method 
leads to a less than one percent improvement in the CDPP on average. Given that this method
is more computationally expensive, we adopt the grid search method outlined above when
producing the \texttt{EVEREST 2.0} catalog.

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.3\textwidth]{figures/crossval.pdf}
       \caption{Cross-validation procedure for first order PLD on EPIC 206103150,
       (WASP-47 e) a campaign 3 planet host. Shown is the scatter $\sigma_v$ in the validation
       set (red) and the scatter in the training set (blue) as a function of $\lambda_1$,
       the prior amplitude for the first order PLD weights, for each of three light curve
       sections; the mean scatter is shown at the bottom. Red arrows indicate the minima
       in the $\sigma_v$ curves for each section; note that because of variable noise
       properties across the campaign, they all occur at different values of $\lambda_1$.
       The dashed vertical line indicates the value of $\hat{\lambda}_n$ obtained by the
       procedure outlined in the text, which establishes a compromise between 
       slight underfitting in the first two segments and slight overfitting in the
       third.
       }
     \label{fig:crossval}
  \end{center}
\end{figure}

In addition to being more computationally tractable, there are two major benefits to
minimizing $\sigma_v$ in this fashion. First, since we perform cross-validation three times
(once for each PLD order), we are able to progressively refine the outlier masks (\S\ref{sec:impl_lightcurves}) and
the GP hyperparameters (\S\ref{sec:impl_gp}) for each target in between cross-validation steps. Second, it
allows for some leeway in how we determine the minimum validation scatter. In Paper I, 
we sought to minimize the \emph{median} scatter in groups of random 13-cadence segments 
of the light curve (the validation set). A potential issue with this method is that 
the noise properties of $K2$ light curves are far from constant over the course of
an observing campaign; optimizing the regression based on the median (or mean) validation 
scatter can still, in principle, lead to overfitting in some segments. While splitting the
light curves into segments with similar noise properties (\S\ref{sec:impl_breakpoints})
helps with this, we also modify the cross-validation process to prevent localized
overfitting. For each PLD order $n$ and for each value of $\lambda_n$, 
we split each light curve segment 
into three roughly equal sections. For each pair of sections, we train the model on
them and compute the model prediction in the third section (the validation set). We
then compute the scatter $\sigma_v$ as the MAD of the
de-trended validation set after removing the GP prediction.

We now have three $\sigma_v(\lambda_n)$ curves, one for each section. In general, the
minima of these curves will occur at different values of $\lambda_n$, so determining
the optimum value $\hat{\lambda}_n$ requires a compromise between overfitting and
underfitting in the different segments. For each segment, we compute the minimum scatter, 
find the set of all $\lambda_n$
for which $\sigma_v(\lambda_n)$ is within 5\% of the minimum, and keep the largest
$\lambda_n$. We then pick $\hat{\lambda}_n$ to be the \emph{smallest} of these
values, provided it is smaller than the largest value of $\lambda_n$ at the minima
of the three $\sigma_v$ curves. This process ensures that $\hat{\lambda}_n$ falls
between the minima of the $\sigma_v$ curves with the smallest and largest value of
$\lambda_n$, and that it leads to no more than 5\% overfitting in
one of the chunks. We illustrate this procedure in Figure~\ref{fig:crossval}, where
we show $\sigma_v(\lambda_1)$ for each of the three light curve sections for
EPIC 206103150. The red arrows indicate the minimum of each of the curves, and the
dashed vertical line indicates the adopted $\hat{\lambda}_n$ based on a compromise
between slight underfitting in the first two segments and slight overfitting in the
third. This results in a more conservative cross-validation process than in Paper I.

\section{Results}
\label{sec:results}

\subsection{Injection Tests}
\label{sec:inj}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/injections.pdf}
       \caption{Transit injection/recovery statistics based on 2,700 randomly selected stars from campaign 6.
       Each panel shows histograms of the number of transits recovered with a certain depth 
       ratio $D/D_0$ (recovered depth divided by true depth). Blue histograms correspond to the actual
       injection and recovery process, in which transits are injected into the raw light curves at the pixel level
       and recovered after de-trending with \texttt{EVEREST}; red histograms correspond to control runs in which the transits
       were injected into the \emph{de-trended} data. The values 
       to the left and right of each histogram are the median $D/D_0$ for our pipeline and for the 
       control run, respectively. The smaller values at the top indicate the fraction of transits recovered 
       with depths lower and higher than the bounds of the plots. Finally, the two columns distinguish between 
       runs in which the transits were explicitly masked prior to de-trending (left) and runs in which they were not (right), 
       while the three rows correspond to different injected depths: $10^{−2}$, $10^{−3}$, and $10^{−4}$. \texttt{EVEREST} 
       preserves transit depths if the transits are properly masked; otherwise, a ${\sim}10\%$ bias toward smaller depths is 
       introduced for transits with low signal-to-noise.}
     \label{fig:injections}
  \end{center}
\end{figure}

As in Paper I, we perform simple transit injection/recovery tests to ensure
our model is not overfitting.

\subsection{Ridge Regression}
\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/ridge_reg.pdf}
       \caption{6-hr CDPP comparison between de-trending with ridge (L2) regression (this paper) and
                de-trending with PCA (Paper I) for a sample of 2,700 randomly selected campaign 6
                stars, as in Figure~\ref{fig:pPLD}. Ridge regression leads to a small CDPP improvement of 
                ${\sim}1$ to 5\%.}
     \label{fig:ridge_reg}
  \end{center}
\end{figure}

\subsection{Neighboring PLD}
\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/nPLD.pdf}
       \caption{6-hr CDPP comparison between PLD de-trending with ridge regression + neighboring targets 
                (this paper) and standard PLD de-trending (Paper I) for the same sample of stars as in Figure~\ref{fig:ridge_reg}.
                Each target was de-trended with its own PLD vectors plus those of ten random bright stars
                on the same module. This method leads to a robust CDPP improvement of ${\sim}10$\% for bright ($\Kp \lesssim 13$) stars
                and ${\sim}20$\% for fainter stars.}
     \label{fig:nPLD}
  \end{center}
\end{figure}

\section{Limitations}
\label{sec:limitations}
How do we do on variable stars? Talk about 201270464.


\section{Comparison to Other Pipelines}
\label{sec:comparison}

\subsection{CDPP}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_everest1_all.pdf}
       \caption{CDPP comparison between \texttt{EVEREST 2.0} and \texttt{EVEREST 1.0} for all stars in campaigns 0--8.
       As before, individual stars are plotted as blue points and the median CDPP is indicated by a black line; note
       the ${\sim}10-20$\% improvement over the previous version of the pipeline. Saturated
       stars are plotted as red points, with their median CDPP indicated by a dashed red line. The apparently
       better performance of \texttt{EVEREST 1.0} for these stars is spurious, since traditional PLD typically
       leads to strong overfitting of saturated stars (see Figure~\ref{fig:saturated_star}).}
     \label{fig:cdpp_everest1_all}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sff_all.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_everest1_all}, but showing a comparison between \texttt{EVEREST 2.0} and \texttt{K2SFF}.
       \texttt{EVEREST 2.0} outperforms \texttt{K2SFF} at all magnitudes, including $\Kp \lesssim 11$, for which stars are saturated.}
     \label{fig:cdpp_k2sff_all}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sc_all.pdf}
       \caption{Similar to the previous figure, but showing a comparison between \texttt{EVEREST 2.0} and \texttt{K2SC}.
       \texttt{EVEREST 2.0} light curves have lower average CDPP at all magnitudes except around $\Kp \approx 9$, for which
       the precision is comparable.}
     \label{fig:cdpp_k2sc_all}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_everest1.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_everest1_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{EVEREST 1.0} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_everest1}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sff.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_k2sff_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{K2SFF} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_k2sff}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sc.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_k2sc_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{K2SC} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_k2sc}
  \end{center}
\end{figure}

\subsection{Comparison to Kepler}

\begin{figure*}[t]
  \begin{center}
      \leavevmode
      \includegraphics[width=\textwidth]{figures/cdpp_kepler.pdf}
       \caption{6-hr photometric precision as a function of \emph{Kepler} magnitude $\Kp$ for all 
       stars observed by \emph{Kepler} (yellow dots) and for all $K2$ targets in Campaigns 0-8
       de-trended with \texttt{EVEREST} (blue). The median in 0.5 magnitude-wide bins is indicated
       by yellow circles for \emph{Kepler} and by blue circles for \texttt{EVEREST}. For campaigns
       1, 5, and 6, \texttt{EVEREST} recovers the raw \emph{Kepler} photometric precision down to
       at least $\Kp = 15$; for campaigns 3, 4, and 8, \texttt{EVEREST} recovers the \emph{Kepler} 
       precision down to $\Kp = 14$. Campaigns 0 and 2 have a larger fraction of (variable) giant
       stars, leading to a higher average CDPP, while campaign 7 raw light curves have significantly
       worse precision due to a change in the orientation of the spacecraft and excess jitter.
       }
     \label{fig:cdpp_kepler}
  \end{center}
\end{figure*}

\subsection{Outliers}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/outliers.pdf}
       \caption{Histograms showing the number of non-outlier data points per campaign for each
        of four pipelines: \texttt{K2SFF} (gray), \texttt{K2SC} (orange), \texttt{EVEREST 1.0}
        (red), and \texttt{EVEREST 2.0} (blue). To compute these, we remove all cadences 
        with flagged \texttt{QUALITY} bits (excepting thruster fires) from all light curves, then
        smooth each light curve with a second order, 2-day Savitsky-Golay filter and perform iterative 
        sigma clipping at 5-$\sigma$ to remove the outliers. The number of remaining cadences is
        for each light curve is then used to plot the histograms. Both versions of \texttt{EVEREST}
        have more usable data points per campaign than the other pipelines. On average, 
        \texttt{EVEREST} light curves have ${\sim}200-300$ more non-outlier data points than \texttt{K2SFF} 
        and ${\sim}100$ more than \texttt{K2SC}.}
     \label{fig:outliers}
  \end{center}
\end{figure}

\section{CBVs}
\label{sec:cbvs}

\begin{figure}[h]
	\centering   
	\subfigure[CBV \#1]{
		\label{fig:cbv1}
		\includegraphics[width=0.2\textwidth]{figures/cbv1.pdf}
	}
	\subfigure[CBV \#2]{
		\label{fig:cbv2}
		\includegraphics[width=0.2\textwidth]{figures/cbv2.pdf}
	}
	\caption{Campaign 2 Co-trending Basis Vectors (CBVs). We apply \texttt{SysRem} to all 
	         de-trended light curves in each of the 19 functioning modules on the \emph{Kepler}
	         CCD. Plotted are the \textbf{(a)} first and \textbf{(b)} second CBVs recovered
	         for each module. Since campaign 2 light curves are de-trended in two separate
	         segments, we apply \texttt{SysRem} to each segment separately (blue and red
	         curves). The first set of CBVs contain primarily linear trends with the hook-like
	         thermal features at the beginning of the campaign; the second set of CBVs are
	         dominated by quadratic trends. We correct all campaign 2 light curves by simple
	         linear regression with the first two CBVs, with the exception of those on module
	         20, whose second CBV signal displays higher frequency oscillations.
	         This module contains fewer stars than the other modules, leading to CBVs that contain
	         a higher amount of astrophysical information. For this module and for modules in 
	         other campaigns whose second CBV is not predominantly quadratic, we correct the
	         light curves with a single CBV. \label{fig:cbv}}
\end{figure}

\bibliographystyle{apj}
\bibliography{everest}
\end{document}