\documentclass[]{emulateapj}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath,mathtools}
\usepackage{epsfig}
\usepackage[FIGTOPCAP]{subfigure}
\usepackage{afterpage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{relsize}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{morefloats}
\usepackage{wasysym}
\usepackage{rotating}

\newcommand{\noop}[1]{}
\newcommand{\note}[1]{{\color{red} #1}}
\newcommand{\cn}{\note{(citation needed)\ }}
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\newcommand{\mearth}{\unit{M_\oplus}}
\newcommand{\rearth}{\unit{R_\oplus}}
\newcommand{\msun}{\unit{M_\odot}}
\newcommand{\lsun}{\unit{L_\odot}}
\newcommand{\mstar}{\unit{M_\star}}
\newcommand{\rj}{\ensuremath{R_\mathrm{J}}}
\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\bavg}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\newcommand{\Kp}{\ensuremath{K_\mathrm{p}}}
\DeclareMathOperator*{\argmin}{arg\,min}

\shorttitle{EVEREST 2.0}
\shortauthors{Luger et al. 2016}

\begin{document}

\title{An update to the EVEREST K2 pipeline:\\ Short cadence, saturated stars, and Kepler-like photometry down to $\Kp = 15$}
\author{Rodrigo Luger\altaffilmark{1,2}, Ethan Kruse\altaffilmark{1}, Eric Agol\altaffilmark{1,2},\\
Daniel Foreman-Mackey\altaffilmark{1,3}, Nicholas Saunders\altaffilmark{1}, Rory Barnes\altaffilmark{1,2}}
\altaffiltext{1}{Astronomy Department, University of Washington, Box 351580, Seattle, WA 98195, USA; \href{mailto:rodluger@uw.edu}{rodluger@uw.edu}}
\altaffiltext{2}{Virtual Planetary Laboratory, Seattle, WA 98195, USA}
\altaffiltext{3}{Sagan Fellow}

\begin{abstract}
We present an update to the \texttt{EVEREST} $K2$ pipeline, aimed at addressing various limitations
in the previous version and improving the photometric precision of the de-trended light curves.
We developed a fast regularization scheme for third order pixel level decorrelation (PLD) and
adapted the algorithm to include the PLD vectors of neighboring stars to enhance the predictive
power of the model and minimize overfitting, particularly for faint stars. We also modified PLD to work for saturated
stars and improved its performance on extremely variable stars. On average, \texttt{EVEREST 2.0}
light curves have 10--20\% higher photometric precision than those in the previous version,
yielding the highest precision light curves at all $\Kp$ magnitudes of any publicly available
pipeline. For most $K2$ campaigns, we recover the original \emph{Kepler} precision to at least
$\Kp = 14$, and to at least $\Kp = 15$ for campaigns 1, 5, and 6. We have also de-trended all short
cadence targets observed by $K2$, obtaining even higher photometric precision for these stars.
All light curves for campaigns 0--8 are available online in the \texttt{EVEREST} catalog, which
will be continuously updated with future campaigns.
\texttt{EVEREST 2.0} is open source and was coded in a general framework that can be applied
to other photometric surveys, including \emph{Kepler} and the upcoming \emph{TESS} mission.
\end{abstract}

\section{Introduction}
\label{sec:intro}
The failure of the second of its four reaction wheels in 2013 brought the original
\emph{Kepler} mission to a premature conclusion, as the spacecraft could no longer
achieve the fine pointing precision required for the groundbreaking
transiting exoplanet and stellar variability science that the first four years of the
mission allowed. Since 2014 the spacecraft has been operating in a new mode, known
as $K2$, using the solar wind and periodic thruster firings to mitigate drift
\citep{Howell14}. However, despite these measures, raw $K2$ photometry is significantly
poorer than that of the original \emph{Kepler} mission. In order to enable
continuing precision science with $K2$, numerous pipelines have been developed
\citep[e.g.,][]{VanderburgJohnson14,Armstrong15,Lund15,Crossfield15,ForemanMackey15,Huang15,Aigrain16},
many producing light curves with precision approaching that of \emph{Kepler}.
Despite significant differences in their implementation, all of these pipelines
regress (whether implicitly or explicitly) on information relating to the motion
of the spacecraft as determined from the collective behavior of hundreds or thousands
of stars on the CCD. Since this information is encoded at high signal-to-noise in
the set of all stars on the detector, these methods excel at mitigating the
instrumental effects due to the spacecraft motion. However, inhomogeneities in the
detector at the pixel level lead to inter- and intra-pixel sensitivity variations
that distort astrophysical information in ways that these methods cannot fully
correct for, particularly when these variations are different across different
pixels or in different locations on the detector.

With this in mind, in \cite{Luger16} (henceforth Paper I), we developed the first version of our 
pipeline, \texttt{EVEREST 1.0}, which regresses exclusively on \emph{local} information.
We employed a variant of pixel level decorrelation (PLD) based on the method of \cite{Deming15},
a data-driven approach that uses a star's own pixel-level light curve to remove
instrumental effects. We showed that \texttt{EVEREST 1.0} recovered the original \emph{Kepler} 
precision for stars brighter than $Kepler$-band magnitude $\Kp \approx 13$, yielding higher 
average precision than any publicly available $K2$ catalog for unsaturated stars.

In this paper, we present an update to our pipeline, which we refer to as \texttt{EVEREST 2.0}.
By combining PLD with spacecraft motion information obtained from nearby stars,
this update improves the precision of $K2$ light curves at all magnitudes relative to version \texttt{1.0} and addresses
certain issues with overfitting. \texttt{EVEREST 2.0} also reliably de-trends saturated
stars and stars observed in short cadence mode, obtaining comparable or even higher de-trending
power for these targets.

The paper is organized as follows: in \S\ref{sec:model} we derive the mathematical
framework of the \texttt{EVEREST 2.0} model, and in \S\ref{sec:implementation} we describe 
the implementation of our pipeline in detail. We present our results in \S\ref{sec:results}
and some general remarks in \S\ref{sec:remarks}. In \S\ref{sec:conclusions} we 
summarize the work and list the \texttt{EVEREST} resources available online.

\section{The PLD Model}
\label{sec:model}
Here we describe the mathematical formulation of the \texttt{EVEREST}
pixel level decorrelation (PLD) model. In PLD, products of the fractional fluxes in each pixel
of the target aperture are used as regressors in a linear model:
%
\begin{align}
\label{eq:pldmodel}
\mathbf{m} = &\sum\limits_{i}                                 a_i     \frac{\mathbf{p}_{i}}                             { \sum\limits_{n}\mathbf{p}_{n}} +     \nonumber\\
             &\sum\limits_{i} \sum\limits_{j}                 b_{ij}  \frac{\mathbf{p}_{i}\mathbf{p}_{j}}               {(\sum\limits_{n}\mathbf{p}_{n})^2} +  \nonumber\\
             &\sum\limits_{i} \sum\limits_{j} \sum\limits_{k} c_{ijk} \frac{\mathbf{p}_{i}\mathbf{p}_{j}\mathbf{p}_{k}} {(\sum\limits_{n}\mathbf{p}_{n})^3}.
\end{align}
%
In the expression above, $\mathbf{m}$ is the model and $\mathbf{p}_{i}$ is the flux in
the $i^{th}$ pixel; both are vector quantities defined at an array of times $\mathbf{t}$.
Each term corresponds to a different PLD order (first, second, and third) resulting from 
a Taylor expansion of the instrumental signal.
The $a_i$, $b_{ij}$, and $c_{ijk}$ are the linear weights of the model, which we seek
to obtain below. For a detailed discussion of the theory behind PLD, see 
\cite{Deming15} and Paper I. Below we simply discuss its mathematical implementation.

\subsection{Ridge Regression (\texttt{rPLD})}
\label{sec:ridge}
Given a timeseries $\mathbf{y}$ with $N_{dat}$ data points, we wish to find the linear
combination of $N_{reg}$ regressors that best fits the instrumental component of $\mathbf{y}$.
Expressed in vector form, our linear model is thus
%
\begin{align}
\label{eq:xdotw}
\mathbf{m} = \mathbf{X} \cdot \mathbf{w},
\end{align}
%
where $\mathbf{X}$ is the ($N_{dat} \times N_{reg}$) design matrix constructed from the set
of regressors (the fractional pixel fluxes in Equation~\ref{eq:pldmodel}) and $\mathbf{w}$ is 
the ($N_{reg} \times 1$) vector of weights (the set
$\{a_i, b_{ij}, c_{ijk}\}$). If $\mathbf{w}$ is known, the de-trended light curve is simply
%
\begin{align}
\label{eq:detrended}
\mathbf{y}' = \mathbf{y} - \mathbf{m}.
\end{align}
%
In Paper I, we obtained $\mathbf{w}$ by maximizing the likelihood function
%
\begin{align}
\label{eq:like0}
\log\mathcal{L}_0 =  &-\frac{1}{2} \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right)^\top
                     \cdot
                     \mathbf{K^{-1}}
                     \cdot
                     \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right) \nonumber\\  
                     %
                     &-\frac{1}{2} \log\left|\mathbf{K}\right| 
                     -\frac{N_{dat}}{2}\log 2\pi,
\end{align}
%
where $\mathbf{K}$ is the ($N_{dat} \times N_{dat}$) covariance matrix of the data and
$\mathbf{y}$ is the ($N_{dat} \times 1$) SAP flux. Since the number of third order PLD regressors can
be quite large (on the order of several thousand for a typical star, which is larger than the number
of data points), the problem is ill-posed, meaning that a unique solution does not exist and 
maximizing $\log\mathcal{L}_0$ is likely to lead to overfitting. We thus constructed $\mathbf{X}$
from the (smaller) set of $N_{pca}$ principal components of the PLD regressors. We chose $N_{pca}$
by performing cross-validation, which aims to maximize the predictive power of the model while
minimizing overfitting.

However, while principal component analysis (PCA) yields a set of components that captures 
the most variance among the PLD vectors, there is no guarantee that the principal components
are the ideal regressors in the PLD problem. Dimensionality reduction techniques such as PCA
inevitably lead to information loss, and so it is worthwhile to consider alternative
regression methods to fully exploit the potential of PLD.

A common regression method for ill-posed problems is regularization, in which a prior is imposed
on the values of the weights $\mathbf{w}$. Since overfitting occurs when $\mathbf{w}$ becomes very
large, regularization recasts the problem by adding a penalty term to the likelihood that 
increases with increasing $|\mathbf{w}|$. While many forms of regularization exist, we focus on
ridge (L2) regression, since it has an analytic solution. Ridge regression involves placing a
Gaussian prior on each of the weights $\mathbf{w}$, so that the posterior likelihood function becomes
%
\begin{align}
\label{eq:like}
\log\mathcal{L} =  \log\mathcal{L}_0
                   -\frac{1}{2}
                   \mathbf{w}^\top \cdot \mathbf{\Lambda}^{-1} \cdot \mathbf{w}
                   -\frac{1}{2} \log\left|\mathbf{\Lambda}\right|,
\end{align}
%
where $\mathbf{\Lambda}$ is the ($N_{reg} \times N_{reg}$) diagonal regularization matrix,
%
\begin{align}
\label{eq:Lambda}
\Lambda_{m,n} = \lambda_{n}^2\delta_{mn}.
\end{align}
%
Each element $\lambda_n^2$ in $\mathbf{\Lambda}$ is the variance of the 
zero-mean Gaussian prior on the weight of the corresponding column of the design matrix, 
$\mathbf{X}_{*,n}$. Provided we choose the $\lambda_{n}$ correctly, this model should 
have a higher predictive power than the PCA model adopted in Paper I.

Given this formulation, our task is to find the weights $\mathbf{\hat{w}}$ that maximize 
the posterior probability 
$\mathcal{L}$. Differentiating 
Equation~(\ref{eq:like}) with respect to $\mathbf{w}$, we get
%
\begin{align}
\label{eq:gradlike}
\frac{\mathrm{d}\mathbf{\log\mathcal{L}}}{\mathrm{d}\mathbf{w}} &= 
%
\mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y} \nonumber\\
%
&- \left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right) \cdot \mathbf{w}.
\end{align}
%
By setting this expression equal to zero, we obtain the maximum \emph{a posteriori} prediction 
for the weights,
%
\begin{align}
\label{eq:what}
\mathbf{\hat{w}} = 
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}
\end{align}
with corresponding model
%
\begin{align}
\label{eq:model_slow}
\mathbf{m} = 
%
\mathbf{X} \cdot
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}.
\end{align}
In what follows, we refer to the implementation of PLD with ridge regression as \texttt{rPLD}.

\subsection{Cross-validation}
\label{sec:crossval}
Similarly to Paper I, we solve for $\mathbf{\Lambda}$ by cross-validation. For each value
of $\mathbf{\Lambda}$, the model is trained on one part of the light curve (the training set)
and used to de-trend the other part of the light curve (the validation set). The value of 
$\mathbf{\Lambda}$ that results in the minimum scatter in the validation set is then chosen for
the final de-trending step. 

In principle, each of the $\lambda_n$ in $\mathbf{\Lambda}$ could take on a different value, 
but solving for each one requires minimizing an $N_{reg}$-dimensional function and is not computationally tractable.
Instead, we simplify the problem by requiring that all regressors of the same order have the
same regularization parameter $\lambda$. Provided we write the third order design matrix in the form
%
\begin{align}
\label{eq:design}
\mathbf{X} = 
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right),
\end{align}
%
where $\mathbf{X_n}$ is the matrix of $n^\mathrm{th}$ order regressors, we may
express the regularization matrix as
%
\begin{align}
\label{eq:Lambda_block}
\mathbf{\Lambda} = 
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}      &                       & \\
  &                       \mathbf{\Lambda_2}      & \\
  &                       &                       \mathbf{\Lambda_3} \\
\end{array}
\right)
\end{align}
%
where $\mathbf{\Lambda_n} = \lambda_{n}^2\mathbf{I}$ is the 
$n^\mathrm{th}$ order regularization matrix and $\lambda_{n}^2$ is the variance
of the prior on the $n^\mathrm{th}$ order regressors. 

A typical $K2$ star with 30 aperture pixels has $N_{reg} \sim\ $5,000 regressors and
$N_{dat} \sim\ $500 data points in each cross-validation light curve segment 
(see \S\ref{sec:impl_crossval}). Evaluating the matrix inverse in Equation~(\ref{eq:model_slow})
is thus computationally expensive, and becomes prohibitive during cross-validation,
since this must be done for every set of $\lambda_{n}$'s. Fortunately, we can reduce 
the number of calculations with some linear algebra. First, we apply the Woodbury matrix 
identity to Equation~(\ref{eq:model_slow}), obtaining
%
\begin{align}
\label{eq:model_woodbury}
\mathbf{m} =      \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top
                  \cdot
                  \left(
                  \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
Next, we note that
%
\begin{align}
\label{eq:separable1}
\mathbf{X} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{\Lambda} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X}^\top &= 
%
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right)
%
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}\hspace*{-8pt}      &                                     & \\[2pt]
  &                                     \mathbf{\Lambda_2}\hspace*{-8pt}      & \\[2pt]
  &                                     &                                     \mathbf{\Lambda_3} \\
\end{array}
\right)
%
\left(
\begin{array}{c}
  \mathbf{X_1^\top} \\[2pt]
  \mathbf{X_2^\top} \\[2pt]
  \mathbf{X_3^\top} \\
\end{array}
\right) \nonumber\\[5pt]
%
&= \lambda_1^2 \mathbf{X_1} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_1} +
   \lambda_2^2 \mathbf{X_2} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_2} +
   \lambda_3^2 \mathbf{X_3} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_3} \nonumber\\[5pt]
%
&= \sum_n \lambda_n^2 \mathbf{X^2_n},
\end{align}
%
where we have defined
%
\begin{align}
\label{eq:x2}
\mathbf{X^2_n} \equiv \mathbf{X_n} \cdot \mathbf{X^\top_n}.
\end{align}
%
We may thus re-write our maximum \emph{a posteriori} model as
%
\begin{align}
\label{eq:model}
\mathbf{m} = \sum_n \lambda_n^2 \mathbf{X^2_n}
                  \cdot
                  \left(
                  \sum_n \lambda_n^2 \mathbf{X^2_n} + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
The matrix that we must invert in Equation~(\ref{eq:model}) has dimensions ($N_{dat} \times N_{dat}$),
while that in Equation~(\ref{eq:model_slow}) is ($N_{reg} \times N_{reg}$). Since
$N_{reg} \sim 10N_{dat}$, casting the model in this form can greatly speed up the
computation. In practice, we pre-compute the three matrices $\mathbf{X^2_n}$ at the beginning
of the cross-validation step, so the only time-consuming operation in Equation~(\ref{eq:model})
is the inversion.

\subsection{Neighboring Stars (\texttt{nPLD})}
\label{sec:neighboring}
The principal difference between PLD and other methods used to correct $K2$ light curves
\citep[e.g.,][]{Vanderburg14,ForemanMackey15,Aigrain16} is in the origin of the information
used in the de-trending. PLD uses \emph{local} information, present in a given star's pixels,
to de-trend that star's light curve; alternative methods almost exclusively use \emph{global} information,
derived from the behaviour of many stars across the detector, to accomplish the same thing. 
It is perhaps surprising that PLD works so well, given that the signal-to-noise ratio (SNR) of the regressors
of the latter methods is \emph{necessarily} higher, since they are derived from hundreds to
thousands of stars and thus contain information about the spacecraft motion at high fidelity.
The fact that PLD outperforms other pipelines --- and in particular its ability to correct
data collected during thruster firings --- suggests that local modes contribute significantly
to the instrumental noise in $K2$ light curves. Local noise is due to inter- and intra-pixel
variability and local pixel response function (PRF) behavior and is specific to the properties of 
the detector at the particular location of the target, while global noise is due solely to 
the motion of the spacecraft and is shared among all targets. Traditional methods can only
correct the latter (but do so extremely well). PLD excels at correcting the former. While
PLD certainly removes the noise introduced by the spacecraft motion (i.e., global noise), its
regressors are noisy, particularly for dimmer targets. This suggests that a combination of
both approaches that explicitly de-trends both local and global noise could substantially improve 
the algorithm, especially for faint stars.

A simple way of implementing this would be to include the two \emph{Kepler} motion vectors (the
\texttt{POS\_CORR1} and \texttt{POS\_CORR2} fields in the target pixel files), as well as
higher order products of these, in the regression, assigning each their own L2 priors ($\mathbf{\Lambda}$).
While this approach works, in practice we obtain higher de-trending power by instead including the PLD
vectors of bright neighboring targets in the regression; we dub this method \texttt{nPLD}, for \emph{n}eighboring
\emph{PLD}. Our third-order design matrix (Equation~\ref{eq:design}) is now
%
\begin{align}
\label{eq:design_nPLD}
\mathbf{X} = 
\left(
\begin{array}{cccccc}
  \mathbf{X_1} & \mathbf{X_1'} & \mathbf{X_2} & \mathbf{X_2'} & \mathbf{X_3} & \mathbf{X_3'}
\end{array}
\right),
\end{align}
%
where $\mathbf{X_n'}$ is the design matrix constructed from the $n^{th}$ order PLD vectors of all the 
neighboring targets. For computational speed, we still solve for a single prior amplitude $\lambda_n$ 
for each PLD order, but in principle one could assign different priors to the neighboring vectors. We
discuss the implementation of \texttt{nPLD} in \S\ref{sec:implementation}.

\section{Implementation}
\label{sec:implementation}

\subsection{Light curves}
\label{sec:impl_lightcurves}
As in Paper I, we downloaded all stars in the $K2$ EPIC catalog with long and/or short cadence
target pixel files and adopted aperture \#15
from the \texttt{K2SFF} catalog \citep{Vanderburg14,VanderburgJohnson14}. We masked all
cadences with \texttt{QUALITY} flags 1-9, 11-14, and 16-17, though we still compute the
model prediction on them. For campaigns 0-2, we remove the background signal as 
described in Paper I; for more recent campaigns, the background is removed by the \emph{Kepler}
team.

Next, we perform iterative sigma clipping to identify and mask outliers at 5$\sigma$.
During each iteration, we compute the linear, unregularized PLD model and smooth it with
a Savitsky-Golay filter \citep{SavitskyGolay64}, then identify outliers based on a 
median absolute deviation (MAD) cut. We implement this outlier-clipping step at the
beginning of each cross-validation step (\S\ref{sec:impl_crossval}), each time computing
the model with a higher PLD order, to progressively refine the outlier mask.

\subsection{GP Optimization}
\label{sec:impl_gp}
In order to compute the covariance matrix $\mathbf{K}$ for each target, we use a Gaussian
process (GP), as we did in Paper I. GP optimization can be costly, especially when
performing model selection over a range of possible kernels and optimizing many
hyperparameters simultaneously. For this reason, in Paper I we cut corners and performed
kernel selection based on fits to the autocorrelation function of the light curve, which
we also used to fix the timescale and/or period of those kernel(s). We then ran a nonlinear
minimizer to optimize the overall amplitude of the GP. In practice, this worked reasonably
well, but often failed for light curves dominated by high frequency stellar variability.
After much experimentation, we decided to forego the kernel selection step in favor of
using a single carefully optimized white kernel plus a Mat\'ern-3/2 kernel:
%
\begin{align}
\mathbf{K}_{ij} = \sigma^2\delta_{ij} + \alpha \left(1 + \sqrt{3(t_i - t_j)^2}\right) e^{-\sqrt{3(t_i - t_j)^2}/\tau},
\end{align}
%
where the hyperparameters $\sigma$, $\alpha$, and $\tau$ are the white noise amplitude,
red noise amplitude, and red noise timescale, respectively, and $t_i$ and $t_j$ correspond
to the timestamps of cadences $i$ and $j$. We initialize the hyperparameters at random
values and run a nonlinear optimizer to solve for the maximum likelihood (Equation~\ref{eq:like}),
keeping the PLD model parameters fixed; we repeat this process several times and retain the 
highest likelihood solution. As with outlier clipping, we progressively optimize the GP
at each of the three cross-validation steps, so that each time we train the GP on a 
light curve that is increasingly dominated by stellar variability (as opposed to instrumental
systematics).

In principle, the quasi-periodic kernels used in Paper I should be better suited to 
handling variable stars, but in practice we find that a properly optimized Mat\'ern-3/2 
kernel is flexible enough to fully capture the variability and prevent PLD overfitting.
We discuss this further in \S\ref{sec:remarks}.

\subsection{Breakpoints}
\label{sec:impl_breakpoints}
Because the instrumental noise properties are quite variable over the course of $K2$ 
campaigns, we find a significant improvement in the de-trending power of our ridge
regression model when we subdivide light curves into two or three segments. This is in contrast to
the PCA approach in Paper I, where we did not find it necessary to split the timeseries.
For all campaigns except 4 and 7, we add a single breakpoint in the light curve near the
mid-campaign point, where the spacecraft roll is at a minimum. For campaigns 4 and 7,
we find it necessary to insert two breakpoints. We cross-validate and de-trend each 
light curve segment separately and mend them at the end. In order to mitigate flux
discontinuities at the breakpoints, we train the model in each segment on an additional
100 cadences past the breakpoint to remove potential edge effects and offset the models
in each segment so that they align at the breakpoint.
While this method removes flux discontinuities, it can introduce discontinuities in the
derivative of the flux, showing up as spurious ``kinks'' in the light curve. We 
remove these in a post-processing step (\S\ref{sec:cbvs}).

\subsection{Neighboring stars}
\label{sec:impl_neighboring}
In principle, the larger the number of neighboring PLD vectors we include in the
\texttt{nPLD} design matrix, the higher the de-trending power of our model. However, adding 
regressors significantly increases computing time, so we would like to instead select
a small set of high SNR regressors that capture most of the spacecraft
motion information. Moreover, since we employ a single prior for all $n^{th}$ order
regressors, adding many foreign PLD vectors effectively results in an exchange of
local for global de-trending power (\S\ref{sec:neighboring}) and in practice results
in poorer quality light curves. After much experimenting, we find that the ideal number
of neighboring stars to include in the design matrix is about ten. We therefore de-trend
each $K2$ target with the aid of the PLD vectors of ten randomly selected bright 
($11 \leq \Kp \leq 13$) stars on the same detector module as the target. To minimize
contamination of the target by outliers in its neighbors' fluxes, we linearly
interpolate over all neighbor data with flagged \texttt{QUALITY} bits. Finally, for
computational reasons, we neglect all cross terms of the form $\prod_{i \neq j} p_i p_j$,
where $p_i$ is the flux in the $i^{th}$ pixel,
when computing the neighbors' PLD vectors. Cross terms typically encode information
specific to the sets of pixels from which they are computed and aid in correcting
features such as thruster firing discontinuities \citep{Luger16}. Cross terms from
stars other than the target in question are therefore of little help in the de-trending
and can be safely neglected.

One potential pitfall of \texttt{nPLD} is that if the PLD assumptions break down for
any of the neighboring targets, the PLD regressors may become contaminated with
astrophysical information from that neighbor. This is not in general an issue, since
overfitting would only occur if an astrophysical signal in the target star and in
its neighbor had the same period and the same phase. However, in the (unlikely) case
that PLD fails for the neighboring star and this star happens to be an eclipsing binary 
or a transiting exoplanet host, it is possible that its transit signals could get imprinted onto
the target star's de-trended light curve, resulting in potential false positive planet
detections down the line. The two cases relevant to $K2$ in which PLD could fail in such
a way are for saturated stars and stars with bright contaminant sources in their
apertures \citep{Luger16}. As we show in \S\ref{sec:impl_saturated} below, it is 
straightforward to adapt PLD to work reliably for saturated stars. But while 
\texttt{EVEREST 2.0} is more robust against overfitting of crowded stars 
(\S\ref{sec:remarks}), highly crowded apertures remain an issue for PLD. When de-trending
with \texttt{nPLD}, we therefore select neighboring stars with no other known sources
in their apertures that are bright enough ($\Delta \Kp < 5$) to contaminate the PLD
vectors.

\subsection{Saturated stars}
\label{sec:impl_saturated}
As discussed in Paper I, PLD typically fails for stars with saturated pixels, resulting
in overfitted light curves with artificially low scatter and suppressed astrophysical 
information (such as transits with significantly shallower depths). This happens because
saturated pixels contain nearly no astrophysical information, as the signal
overflows into adjacent pixels in the same column and is ultimately dumped into the
pixels at the top and bottom of the bleed trails; these tail pixels ultimately contain more 
astrophysical information than the other pixels in the aperture. Since PLD implicitly
assumes that astrophysical information is constant across the aperture, the method
breaks down for these stars, and PLD vectors from pixels in the saturated columns become capable
of fitting out the astrophysical information in the rest of the aperture.

We suggested in Paper I that collapsing saturated columns into single pixels --- by co-adding
the fluxes in each of the pixels and treating the resulting timeseries as a single PLD pixel ---
could negate the effect of saturation, since charge is conserved along the bleed trail. While
this ensures PLD does not overfit, it leads to the loss of some of the information about the 
vertical motion of the stellar PSF across the detector. This leads to significantly poorer
de-trending, and therefore we did not employ this method in the first version of the pipeline.
However, we find that including the PLD vectors of neighboring stars in the design matrix 
(i.e., \texttt{nPLD}) effectively restores the information lost when saturated columns are 
collapsed, leading to high quality de-trended light curves of saturated stars. 

In practice, we collapse all columns containing one or more pixels whose flux 
comes within 10\% of (or exceeds)
the pixel well depth for the corresponding detector channel during more than 2.5\% of the
timeseries. We obtained the well depths from
Table 13 of the Kepler Instrument Handbook.
\footnote{\url{archive.stsci.edu/kepler/manuals/KSCI-19033-001.pdf}} 

As an example, in Figure~\ref{fig:saturated_star} we plot the light curve of EPIC 202063160, a saturated
campaign 0 eclipsing binary. The raw light curve is shown at the top and the light curve
de-trended with \texttt{EVEREST 1.0} is shown at the center. Because three of the columns
in the aperture contain saturated pixels (right panel), \texttt{EVEREST 1.0} almost 
completely fits out the eclipses. With column-collapsed \texttt{nPLD} (bottom), the
eclipse is preserved and the instrumental signal is effectively removed.

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/saturated_star.pdf}
       \caption{EPIC 202063160, a saturated \Kp = 9.2 campaign 0 eclipsing
       binary. Shown is a portion of the raw light curve (top), the light curve ``de-trended''
       with \texttt{EVEREST 1.0} (center), and the light curve de-trended with \texttt{EVEREST 2.0}
       (bottom). The pixel image is shown at the right on a linear scale, with the adopted aperture
       contour indicated in red. The three columns highlighted in red contain saturated
       pixels. Despite a great improvement in the precision, \texttt{EVEREST 1.0} leads to severe
       overfitting, causing the eclipses to all but disappear. By collapsing saturated columns,
       \texttt{EVEREST 2.0} correctly de-trends saturated stars without overfitting.}
     \label{fig:saturated_star}
  \end{center}
\end{figure}

\subsection{Short cadence}
\label{sec:impl_shortcad}
We treat short cadence targets in much the same way as long cadence targets, with the exception
that we find it necessary to introduce more breakpoints in the light curves. This is
due primarily to computational reasons (short cadence $K2$ light curves are over $10^5$ cadences
in length; computing Equation~(\ref{eq:model}) for the entire light curve is not feasible).
Moreover, we find that noise on sub-30 minute timescales is only properly removed when
the size of the light curve segments is kept small. In practice, we find that on the order of
30 breakpoints result in the best de-trending. This might raise concerns of overfitting, but
since short cadence light curves contains 30 times more data than long cadence light curves,
and we split the latter into two segments, each of the short cadence segments has about twice
as many cadences as the long cadence ones.

The major downside of such a large number of segments are the discontinuities that could
be introduced at each breakpoint. As before, we overcompute the model into adjacent
segments and match the models at the breakpoints, but some de-trended light curves display
occasional jumps in either the flux or its derivative.

A second issue with short cadence light curves concerns deep transits and eclipses. As
we discussed in Paper I, PLD may attempt to fit out these features if they are not
properly masked, since doing so can result in a very large (but spurious) improvement in the
photometric precision.
With long cadence light curves, transit masking can be done by the user by simply re-computing
the model with the appropriate cadences masked, since the transits are sparse and their
presence does not significantly affect the cross-validation step. Moreover, outlier clipping
usually masks most deep transits anyways, so this is hardly ever a problem.
However, that is not the case with short cadence light curves, where transits and eclipses 
span upwards of fifty contiguous cadences. Since these features are so smooth, they are
not flagged as outliers. And since the transit signal is no longer sparse --- as it makes
up a substantial fraction of the light curve segment --- it is far more likely to bias
the cross-validation step. In practice, we find that this leads to substantial 
\emph{under}fitting of short cadence light curves with deep transits. As $\lambda_n$
increases, PLD begins to fit out the transit and the scatter in the validation set
grows, forcing the algorithm to select very low values of $\hat{\lambda}_n$ and resulting
in de-trended light curves that still contain significant instrumental signals.

We therefore explicitly mask all deep transits and eclipses in the short cadence light
curves \emph{before} the cross-validation step. Since only deep transits are likely to
bias the cross-validation, and since the number of short cadence light curves in each
campaign is relatively small, these can easily be identified by inspection.

\subsection{Cross-validation}
\label{sec:impl_crossval}
The principal step in the de-trending process is determining the prior amplitudes $\lambda_n$ in
Equation~(\ref{eq:model}), which we do by cross-validation. Our method is analogous to
that of Paper I, where we performed cross-validation to obtain the optimal number of principal
components to regress on. However, here we seek to optimize a three-dimensional function
$\sigma_\mathrm{v}(\lambda_1, \lambda_2, \lambda_3)$, where $\sigma_\mathrm{v}$ is the scatter 
in the validation set; this is a far more expensive calculation to do.
While we could employ a nonlinear optimization
algorithm (see below), in the interest of computational speed, we perform a simplification. Since we
expect the first order PLD regressors to contain most of the de-trending information,
with each successive PLD order providing a small correction term to the fit, we break
down the minimization problem into three separate one-dimensional problems.
First, we perform cross-validation on the first order PLD model by setting $\lambda_2 = \lambda_3 = 0$
to obtain the value of $\lambda_1$ that minimizes the validation scatter, $\hat{\lambda}_1$. We do this by computing the model
for each value of $\lambda_1$ in a logarithmically-spaced grid with 36 points in the range $[10^{0}, 10^{18}]$,
plus $\lambda_1 = 0$, and select the minimum (details below).
We then repeat this process on the second order model
by fixing $\lambda_1$ at this estimate and keeping $\lambda_3 = 0$. Finally, we solve
for $\hat{\lambda}_3$ by fixing the first and second order parameters at their optimum
values:
%
\begin{align}
\hat{\lambda}_1 &= \argmin \sigma_\mathrm{v}(\lambda_1)\bigg|_{\lambda_2 = 0,\               \lambda_3 = 0} \nonumber\\
\hat{\lambda}_2 &= \argmin \sigma_\mathrm{v}(\lambda_2)\bigg|_{\lambda_1 = \hat{\lambda}_1,\ \lambda_3 = 0} \nonumber\\
\hat{\lambda}_3 &= \argmin \sigma_\mathrm{v}(\lambda_3)\bigg|_{\lambda_1 = \hat{\lambda}_1,\ \lambda_2 = \hat{\lambda}_2}
\end{align}
% 
It is important to note that there is no \emph{a priori} reason
that this method should yield the global minimum of $\sigma_\mathrm{v}$; in fact, it very
likely does not. However, we explicitly allow for $\lambda_n = 0$ in our grid search, and thus 
this approximation cannot lead to overfitting, as it will always prefer a lower-order PLD model to one
with higher scatter in the validation set. 

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/pPLD.pdf}
       \caption{6 hr CDPP comparison between de-trending with \texttt{pPLD} and
                de-trending with \texttt{nPLD} for a sample of 2,700 randomly selected campaign 6
                stars. Plotted is the star-by-star difference in the CDPP values for each method,
                normalized to the \texttt{nPLD} CDPP (blue dots); stars with negative values have lower CDPP 
                when de-trended with \texttt{pPLD}. The black line is the median CDPP difference
                in 0.5 magnitude-wide bins. \texttt{pPLD} leads to an average improvement
                in the CDPP of ${\lesssim}1\%$.}
     \label{fig:pPLD}
  \end{center}
\end{figure}

As a proof of concept, we de-trended a sample of 2,700 randomly selected campaign 6 targets
with \texttt{nPLD} using this approximation in the cross-validation step. 
We then repeated the de-trending by
solving for the $\hat{\lambda}_n$ using Powell's method, initializing the solver at different
points in the vicinity of the \texttt{nPLD} solution and keeping the solution with the lowest
average 6 hr CDPP \citep[combined differential photometric precision;][]{Christiansen12} for each target; we dub this method 
\texttt{pPLD}. In Figure~\ref{fig:pPLD} we plot
the star-by-star CDPP difference between the two models, 
$\mathrm{(CDPP_{pPLD} - CDPP_{nPLD})/CDPP_{nPLD}}$. While for some stars the CDPP improves
substantially with \texttt{pPLD}, cross-validating with Powell's method 
leads to a less than one percent improvement in the CDPP on average. Given that this method
is more computationally expensive, we adopt the grid search method outlined above when
producing the \texttt{EVEREST 2.0} catalog.

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.3\textwidth]{figures/crossval.pdf}
       \caption{Cross-validation procedure for first order PLD on EPIC 206103150
       (WASP-47 e), a campaign 3 planet host. Shown is the scatter $\sigma_v$ in the validation
       set (red) and the scatter in the training set (blue) as a function of $\lambda_1$,
       the prior amplitude for the first order PLD weights, for each of three light curve
       sections; the mean scatter is shown at the bottom. Red arrows indicate the minima
       in the $\sigma_v$ curves for each section; note that because of variable noise
       properties across the campaign, they all occur at different values of $\lambda_1$.
       The dashed vertical line indicates the value of $\hat{\lambda}_n$ obtained by the
       procedure outlined in the text, which establishes a compromise between 
       slight underfitting in the first two segments and slight overfitting in the
       third.
       }
     \label{fig:crossval}
  \end{center}
\end{figure}

In addition to being more computationally tractable, there are two major benefits to
minimizing $\sigma_v$ in this fashion. First, since we perform cross-validation three times
(once for each PLD order), we are able to progressively refine the outlier masks (\S\ref{sec:impl_lightcurves}) and
the GP hyperparameters (\S\ref{sec:impl_gp}) for each target in between cross-validation steps. Second, it
allows for some leeway in how we determine the minimum validation scatter. In Paper I, 
we sought to minimize the \emph{median} scatter in groups of random 13-cadence segments 
of the light curve (the validation set). A potential issue with this method is that 
the noise properties of $K2$ light curves are far from constant over the course of
an observing campaign; optimizing the regression based on the median (or mean) validation 
scatter can still, in principle, lead to overfitting in some segments. While splitting the
light curves into segments with similar noise properties (\S\ref{sec:impl_breakpoints})
helps with this, we also modify the cross-validation process to prevent localized
overfitting. For each PLD order $n$ and for each value of $\lambda_n$, 
we split each light curve segment 
into three roughly equal sections. For each pair of sections, we train the model on
them and compute the model prediction in the third section (the validation set). We
then compute the scatter $\sigma_v$ as the MAD of the
de-trended validation set after removing the GP prediction.

We now have three $\sigma_v(\lambda_n)$ curves, one for each section. In general, the
minima of these curves will occur at different values of $\lambda_n$, so determining
the optimum value $\hat{\lambda}_n$ requires a compromise between overfitting and
underfitting in the different segments. For each segment, we compute the minimum scatter, 
find the set of all $\lambda_n$
for which $\sigma_v(\lambda_n)$ is within 5\% of the minimum, and keep the largest
$\lambda_n$. We then pick $\hat{\lambda}_n$ to be the \emph{smallest} of these
values, provided it is smaller than the largest value of $\lambda_n$ at the minima
of the three $\sigma_v$ curves. This process ensures that $\hat{\lambda}_n$ falls
between the minima of the $\sigma_v$ curves with the smallest and largest value of
$\lambda_n$, and that it leads to no more than 5\% overfitting in
one of the chunks. We illustrate this procedure in Figure~\ref{fig:crossval}, where
we show $\sigma_v(\lambda_1)$ for each of the three light curve sections for
EPIC 206103150. The red arrows indicate the minimum of each of the curves, and the
dashed vertical line indicates the adopted $\hat{\lambda}_n$ based on a compromise
between slight underfitting in the first two segments and slight overfitting in the
third. This results in a more conservative cross-validation process than in Paper I.

\section{Results}
\label{sec:results}

We de-trended all campaigns 0--8 stars with \texttt{nPLD} to produce the 
\texttt{EVEREST 2.0} catalog. Below we report our results, starting with
injection/recovery tests and a comparison of \texttt{rPLD} and \texttt{nPLD},
followed by comparisons with other pipelines and the original \emph{Kepler}
light curves. We report most of our results in terms of the proxy 6 hr CDPP of
the de-trended light curves, which we calculate in the same way as we did
in Paper I: we smooth the light curves with a Savitsky-Golay filter, clip
outliers at 5$\sigma$, and compute the median standard deviation in 13-cadence
chunks, normalized by $\sqrt{13}$.

\subsection{Injection tests}
\label{sec:inj}

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/injections.pdf}
       \caption{Transit injection/recovery statistics based on 2,700 randomly selected stars from campaign 6.
       Each panel shows histograms of the number of transits recovered with a certain depth 
       ratio $D/D_0$ (recovered depth divided by true depth). Blue histograms correspond to the actual
       injection and recovery process, in which transits are injected into the raw light curves at the pixel level
       and recovered after de-trending with \texttt{EVEREST}; red histograms correspond to control runs in which the transits
       were injected into the \emph{de-trended} data. The values 
       to the left and right of each histogram are the median $D/D_0$ for our pipeline and for the 
       control run, respectively. The smaller values at the top indicate the fraction of transits recovered 
       with depths lower and higher than the bounds of the plots. Finally, the two columns distinguish between 
       runs in which the transits were not explicitly masked prior to de-trending (left) and runs in which they were (right), 
       while the three rows correspond to different injected depths: $10^{−2}$, $10^{−3}$, and $10^{−4}$. \texttt{EVEREST} 
       preserves transit depths if the transits are properly masked; otherwise, a ${\sim}10\%$ bias toward smaller depths is 
       introduced for transits with low SNR.}
     \label{fig:injections}
  \end{center}
\end{figure}

As in Paper I, we perform simple transit injection/recovery tests to ensure
our model is not overfitting. For the same sample of 2,700 campaign 6 stars
as before, we injected synthetic transits of varying depths at the raw pixel level
and attempt to recover them after de-trending with \texttt{nPLD}. We follow
the exact same procedure as in \S4.1 of Paper I and plot the results in
Figure~\ref{fig:injections} (compare to Figure~6 in Paper I).

Each panel displays two histograms: a blue one, showing the number of
transits recovered with a certain depth after de-trending with \texttt{nPLD}, 
and a red one, corresponding to a control run in which the transits were injected
into the already de-trended light curve. Each row corresponds to a different
injection depth $D_0$ ($10^{-2}$, $10^{-3}$, and $10^{-4}$, from top to bottom), and the $x$ axis
in each histogram is the recovered depth $D$ scaled to this value ($D/D_0$). The 
left column corresponds to runs in which the transits were not explicitly masked
during de-trending; the right column shows runs in which they were.

As with the previous version of the pipeline, we find a ${\sim}10\%$ bias toward
smaller depths for low SNR transits when the transits are not explicitly masked. This
is because a small decrease in the transit depth can greatly improve the CDPP
of the light curve. Because the PLD regressors are noisy, the method is capable
of partially fitting out transits by exploiting linear combinations of white noise
in the regressors. This overfitting does not occur for high SNR transits because these
are masked during the outlier clipping step.

Conversely, when transits are explicitly masked, there is no bias in the recovered
depth; the median $D/D_0$ is consistent with unity for all three values of the injected 
depth. This is the same result we obtained with \texttt{EVEREST 1.0}, and we conclude
that our new cross-validation scheme is robust in preventing overfitting when transits
are masked. As before, we urge those using \texttt{EVEREST} light curves containing
transits or eclipses to re-compute the model with those features masked. This process
is quick and straightforward --- refer to the \texttt{EVEREST 2.0} documentation.
\footnote{https://github.com/rodluger/everest}

\subsection{\texttt{rPLD}}
\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/ridge_reg.pdf}
       \caption{6 hr CDPP comparison between de-trending with ridge regression (\texttt{rPLD}, this paper) and
                de-trending with PCA (Paper I) for a sample of 2,700 randomly selected campaign 6
                stars, as in Figure~\ref{fig:pPLD}. Ridge regression leads to a small CDPP improvement of 
                ${\sim}1$ to 5\%.}
     \label{fig:rPLD}
  \end{center}
\end{figure}

In Figure~\ref{fig:rPLD} we plot a comparison of the CDPP values obtained with \texttt{rPLD}
and \texttt{EVEREST 1.0} for our sample set of 2,700 campaign 6 stars. As before, the $y$
axis corresponds to the normalized relative CDPP of each model, with negative values
corresponding to lower CDPP for \texttt{rPLD}. Each star is plotted as a blue dot and the median
relative CDPP is indicated as a black line. \texttt{rPLD} outperforms \texttt{EVEREST 1.0} at all
Kepler magnitudes by ${\sim}1-6\%$ on average. However, the scatter at any value of
$\Kp$ is quite large, and the two models are roughly comparable for bright stars. As we
argued in \S\ref{sec:impl_crossval}, the most important feature of \texttt{rPLD} is 
its increased robustness to local overfitting (see \S\ref{sec:remarks}).

\subsection{\texttt{nPLD}}
\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/nPLD.pdf}
       \caption{6 hr CDPP comparison between PLD de-trending with ridge regression + neighboring targets 
                (this paper) and standard PLD de-trending (Paper I) for the same sample of stars as in Figure~\ref{fig:rPLD}.
                Each target was de-trended with its own PLD vectors plus those of ten random bright stars
                on the same module. This method leads to a robust CDPP improvement of ${\sim}10$\% for bright ($\Kp \lesssim 13$) stars
                and ${\sim}20$\% for fainter stars.}
     \label{fig:nPLD}
  \end{center}
\end{figure}

The greatest improvement in the CDPP comes when neighboring stars' PLD vectors are included
in the design matrix. In Figure~\ref{fig:nPLD} we plot the CDPP comparison between \texttt{nPLD}
and \texttt{EVEREST 1.0}. \texttt{nPLD} outperforms regular PLD by ${\sim}10-20\%$
on average, with the largest improvement occurring for fainter stars. Faint stars have the
noisiest PLD vectors and benefit the most from the inclusion of higher SNR regressors.
As we showed in Paper I, regular PLD already comes close to recovering \emph{Kepler} photometric 
precision for bright ($\Kp \lesssim 13$) stars, so the improvement for these stars is naturally 
smaller.

\subsection{Comparison to Other Pipelines}
\label{sec:comparison}

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_everest1_all.pdf}
       \caption{CDPP comparison between \texttt{EVEREST 2.0} and \texttt{EVEREST 1.0} for all stars in campaigns 0--8.
       As before, individual stars are plotted as blue points and the median CDPP is indicated by a black line; note
       the ${\sim}10-20$\% improvement over the previous version of the pipeline. Saturated
       stars are plotted as red points, with their median CDPP indicated by a dashed red line. The apparently
       better performance of \texttt{EVEREST 1.0} for these stars is spurious, since traditional PLD typically
       leads to strong overfitting of saturated stars (see Figure~\ref{fig:saturated_star}).}
     \label{fig:cdpp_everest1_all}
  \end{center}
\end{figure}

In this section we compare the \texttt{EVEREST 2.0} catalog to those produced by other pipelines, beginning with the
previous version of \texttt{EVEREST}.

Figure~\ref{fig:cdpp_everest1_all} shows the CDPP comparison between \texttt{EVEREST 2.0} and
\texttt{EVEREST 1.0} for all campaigns 0--8 stars. As in the example shown in Figure~\ref{fig:nPLD},
\texttt{EVEREST 2.0} outperforms \texttt{EVEREST 1.0} by ${\sim}20\%$ for the faintest stars and by
${\sim}10\%$ for $\Kp \gtrsim 12$. For $11 \lesssim \Kp \lesssim 12$ the two pipelines yield
comparable results, though ridge regression gives \texttt{EVEREST 2.0} a slight edge. Below
$\Kp \approx 11$, $K2$ stars become saturated; these are plotted as red dots, and the median
relative CDPP for saturated stars is indicated by the dashed red line. For these stars,
\texttt{EVEREST 1.0} yields much lower CDPP --- for $\Kp \lesssim 10$, the CDPP is over a factor
of two smaller than that of \texttt{EVEREST 2.0}. As we discussed in Paper I, the increased
performance of \texttt{EVEREST 1.0} for saturated stars is spurious, since the astrophysical
information content of the pixels is highly variable across the aperture, leading regular
PLD to overfit. As we showed in \S\ref{sec:impl_saturated}, \texttt{EVEREST 2.0} does not 
overfit saturated stars. In \S\ref{sec:kepler} below, we show that we approximately recover
the \emph{Kepler} photometric precision for these stars.

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sff_all.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_everest1_all}, but showing a comparison between \texttt{EVEREST 2.0} and \texttt{K2SFF}.
       \texttt{EVEREST 2.0} outperforms \texttt{K2SFF} at all magnitudes, including $\Kp \lesssim 11$, for which stars are saturated.}
     \label{fig:cdpp_k2sff_all}
  \end{center}
\end{figure}

In Figure~\ref{fig:cdpp_k2sff_all} we show the CDPP comparison between \texttt{EVEREST 2.0}
and \texttt{K2SFF} \citep{Vanderburg14,VanderburgJohnson14}. Our pipeline yields
lower average CDPP at all magnitudes, by 40\% for faint stars and 10--20\% for bright
stars. For saturated stars, \texttt{EVEREST 2.0} outperforms \texttt{K2SFF} by 5--10\%.

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sc_all.pdf}
       \caption{Similar to the previous figure, but showing a comparison between \texttt{EVEREST 2.0} and \texttt{K2SC}.
       \texttt{EVEREST 2.0} light curves have lower average CDPP at all magnitudes except around $\Kp \approx 9$, for which
       the precision is comparable.}
     \label{fig:cdpp_k2sc_all}
  \end{center}
\end{figure}

In Figure~\ref{fig:cdpp_k2sc_all} we show the comparison to the \texttt{K2SC} PDC
light curves \citep{Aigrain15,Aigrain16}. \texttt{EVEREST 2.0} yields lower average CDPP at all
magnitudes $K_p \gtrsim 9$, with a 20--25\% improvement for
all unsaturated stars. For $K_p \approx 9$, \texttt{K2SC} slightly overperforms
\texttt{EVEREST 2.0}, but the scatter in the plot is quite large.

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_everest1.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_everest1_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{EVEREST 1.0} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_everest1}
  \end{center}
\end{figure}

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sff.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_k2sff_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{K2SFF} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_k2sff}
  \end{center}
\end{figure}

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sc.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_k2sc_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{K2SC} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_k2sc}
  \end{center}
\end{figure}

We also computed the relative CDPP for each of the campaigns individually; these are
plotted in Figures~\ref{fig:cdpp_everest1}--\ref{fig:cdpp_k2sc}. The improvement
over \texttt{EVEREST 1.0} is approximately the same for all campaigns despite significant
differences in the noise properties and stellar populations across the nine campaigns,
showcasing the robustness of the \texttt{nPLD} method.
The same is true when compared to \texttt{K2SFF} and \texttt{K2SC}, with the exception of
campaigns 0--2, for which \texttt{EVEREST 2.0} outperforms \texttt{K2SFF} by nearly 50\%
(i.e., a factor of 2) at all magnitudes.

\subsection{Comparison to Kepler}
\label{sec:kepler}

\begin{figure*}[hbt]
  \begin{center}
      \includegraphics[width=\textwidth]{figures/cdpp_kepler.pdf}
       \caption{6 hr photometric precision as a function of \emph{Kepler} magnitude $\Kp$ for all 
       stars observed by \emph{Kepler} (yellow dots) and for all $K2$ targets in Campaigns 0-8
       de-trended with \texttt{EVEREST} (blue). The median in 0.5 magnitude-wide bins is indicated
       by yellow circles for \emph{Kepler} and by blue circles for \texttt{EVEREST}. For campaigns
       1, 5, and 6, \texttt{EVEREST} recovers the raw \emph{Kepler} photometric precision down to
       at least $\Kp = 15$; for campaigns 3, 4, and 8, \texttt{EVEREST} recovers the \emph{Kepler} 
       precision down to $\Kp = 14$. Campaigns 0 and 2 have a larger fraction of (variable) giant
       stars, leading to a higher average CDPP, while campaign 7 raw light curves have significantly
       worse precision due to a change in the orientation of the spacecraft and excess jitter.
       }
     \label{fig:cdpp_kepler}
  \end{center}
\end{figure*}

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_kepler_all.pdf}
       \caption{The same as Figure~\ref{fig:cdpp_kepler}, but comparing the CDPP of \emph{all} $K2$ stars
                to that of \emph{Kepler}. \texttt{EVEREST 2.0} recovers the original \emph{Kepler} 
                photometric precision down to at least $\Kp = 14$, and past $\Kp = 15$ for some
                campaigns.}
     \label{fig:cdpp_kepler_all}
  \end{center}
\end{figure}

In Figures~\ref{fig:cdpp_kepler} and \ref{fig:cdpp_kepler_all} we compare the \texttt{EVEREST 2.0}
photometric precision to that of the original \emph{Kepler} mission. Figure~\ref{fig:cdpp_kepler}
shows the CDPP as a function of $\Kp$ for each of the first nine $K2$ campaigns and 
Figure~\ref{fig:cdpp_kepler_all} shows the comparison for all $K2$ stars. Because of differences
in the raw photometric precision and in the stellar populations across the campaigns, the
results are variable, but for all campaigns except 0, 2 and 7, we recover the original \emph{Kepler}
precision down to at least $\Kp = 14$. For campaigns 1, 5, and 6, we recover the \emph{Kepler}
precision down to at least $\Kp = 15$. This also applies to saturated stars, though in some
campaigns the \texttt{EVEREST 2.0} CDPP is slightly higher for these stars. For stars
dimmer than $\Kp = 15$, the \texttt{EVEREST 2.0} CDPP is within a few tens of percent ---
or less --- than that of \emph{Kepler}.

\subsection{Outliers}

\begin{figure}[hbt]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/outliers.pdf}
       \caption{Histograms showing the number of non-outlier data points per campaign for each
        of four pipelines: \texttt{K2SFF} (gray), \texttt{K2SC} (orange), \texttt{EVEREST 1.0}
        (red), and \texttt{EVEREST 2.0} (blue). To compute these, we remove all cadences 
        with flagged \texttt{QUALITY} bits (excepting thruster fires) from all light curves, then
        smooth each light curve with a second order, 2-day Savitsky-Golay filter and perform iterative 
        sigma clipping at 5$\sigma$ to remove the outliers. The number of remaining cadences is
        for each light curve is then used to plot the histograms. Both versions of \texttt{EVEREST}
        have more usable data points per campaign than the other pipelines. On average, 
        \texttt{EVEREST} light curves have ${\sim}200-300$ more non-outlier data points than \texttt{K2SFF} 
        and ${\sim}100$ more than \texttt{K2SC}.}
     \label{fig:outliers}
  \end{center}
\end{figure}

In addition to yielding lower average CDPP than any other publicly available pipeline,
\texttt{EVEREST} yields the largest number of \emph{usable} data points for any of the
$K2$ campaigns to date. These data points generally appear as outliers even in the
raw data, since they are highly sensitive to inter- and intra-pixel sensitivity variations.
Pipelines that regress on functions of the spacecraft motion alone therefore have trouble
de-trending them, resulting in ${\sim}5-10\%$ of the data points being discarded as outliers.
In contrast, since PLD uses regressors containing both inter- and intra-pixel sensitivity
information, it naturally de-trends data collected during thruster firing events (see Paper I).

To show this, we calculated the number of non-outlier data points per campaign
for each of the four pipelines (\texttt{K2SFF}, \texttt{K2SC}, \texttt{EVEREST 1.0}, and
\texttt{EVEREST 2.0}). After removing all data points with flagged \texttt{QUALITY}
bits 1-9, 11-14, and 16-17, we smoothed each light curve with a Savitsky-Golay filter and
performed iterative sigma clipping to remove all 5$\sigma$ outliers. In Figure~\ref{fig:outliers}
we plot histograms of the number of remaining data points for all stars in each of the first
nine campaigns. As expected, both \texttt{EVEREST 1.0} and \texttt{EVEREST 2.0} have, on 
average, 100--300 more usable data points than the other two pipelines. This roughly
corresponds to the number of thruster firings per campaign, as these happen every 6--12
hours on average.

\subsection{Short cadence}
\label{sec:shortcad}
Short cadence results.

\subsection{CBVs}
\label{sec:cbvs}
One downside of the algorithm employed by \texttt{EVEREST} is that GP regression has trouble
distinguishing between low frequency stellar variability and low frequency instrumental
systematics.

\begin{figure}[h]
	\centering   
	\subfigure[CBV \#1]{
		\label{fig:cbv1}
		\includegraphics[width=0.2\textwidth]{figures/cbv1.pdf}
	}
	\subfigure[CBV \#2]{
		\label{fig:cbv2}
		\includegraphics[width=0.2\textwidth]{figures/cbv2.pdf}
	}
	\caption{Campaign 2 Co-trending Basis Vectors (CBVs). We apply \texttt{SysRem} to all 
	         de-trended light curves in each of the 19 functioning modules on the \emph{Kepler}
	         CCD. Plotted are the \textbf{(a)} first and \textbf{(b)} second CBVs recovered
	         for each module. Since campaign 2 light curves are de-trended in two separate
	         segments, we apply \texttt{SysRem} to each segment separately (blue and red
	         curves). The first set of CBVs contain primarily linear trends with the hook-like
	         thermal features at the beginning of the campaign; the second set of CBVs are
	         dominated by quadratic trends. We correct all campaign 2 light curves by simple
	         linear regression with the first two CBVs, with the exception of those on module
	         20, whose second CBV signal displays higher frequency oscillations.
	         This module contains fewer stars than the other modules, leading to CBVs that contain
	         a higher amount of astrophysical information. For this module and for modules in 
	         other campaigns whose second CBV is not predominantly quadratic, we correct the
	         light curves with a single CBV. \label{fig:cbv}}
\end{figure}

\subsection{Sample light curves}
\label{sec:sample}
Here are some sample light curves.

\section{General Remarks}
\label{sec:remarks}
How do we do on variable stars? Talk about 201270464.
How do we do on crowded fields?

\section{Conclusions}
\label{sec:conclusions}
We have presented \texttt{EVEREST 2.0}, an update to the \texttt{EVEREST} pipeline
\citep{Luger16} for removing instrumental noise from $K2$ photometry. In version
\texttt{1.0}, we constructed a linear model from the principal components of
products of the fractional pixel fluxes in the aperture of each star, a variant
of a method known as pixel level decorrelation \citep[PLD,][]{Deming15}. Here,
we regress on \emph{all} PLD vectors, imposing Gaussian priors on the model
weights to prevent overfitting. We additionally include the PLD vectors of bright
neighboring stars to increase the signal-to-noise ratio of the regressors and
enhance the predictive power of the model. We developed a fast gaussian process (GP)
regression scheme to de-trend all stars in the $K2$ catalog, achieving lower
combined differential photometric precision (CDPP) than in version \texttt{1.0},
by ${\sim}10\%$ for bright stars and ${\sim}20\%$ for faint stars. We also
adapted PLD to work for saturated stars, yielding comparable de-trending power,
and stars observed in short cadence mode, yielding higher photometric precision
on 6 hr timescales than their long cadence counterparts.
We further find that the inclusion of neighboring PLD vectors and a
more conservative cross-validation scheme enhance the pipeline's robustness
to overfitting, particularly for highly variable stars.

\texttt{EVEREST 2.0} light curves have higher photometric precision than the two
other publicly available catalogs, \texttt{K2SFF} \citep{VanderburgJohnson14}
and \texttt{K2SC} \citep{Aigrain16}, at all $\Kp$ magnitudes. For faint stars,
\texttt{EVEREST 2.0} has ${\sim}40\%$ lower CDPP than \texttt{K2SFF} and
${\sim}25\%$ lower CDPP than \texttt{K2SC}; for bright unsaturated stars, the CDPP
improvement is ${\sim}20\%$ compared to both pipelines. For saturated stars,
\texttt{EVEREST} outperforms both pipelines, but by a smaller margin. We also
find that \texttt{EVEREST} light curves have, on average, 100--300 fewer outliers
than those of other pipelines, owing primarily to the ability of PLD to correct
data collected during thruster firing events.

When compared to the original \emph{Kepler} mission, \texttt{EVEREST 2.0}
recovers \emph{Kepler} photometry on average to $\Kp \approx 14.5$, and past
$\Kp = 15$ for some campaigns. For dimmer stars, the CDPP is within a few tens
of percent of that of \emph{Kepler}. \texttt{EVEREST} light curves should thus enable 
continued high-precision transiting exoplanet and stellar variability science 
for the vast majority of \emph{K2} stars as if they had been observed by the original
four-wheeled mission.

As with the previous version of the code, \texttt{EVEREST 2.0} is open source under
the MIT license and available at \url{https://github.com/rodluger/everest}, with a 
static release of the code used to generate the catalog archived at 
\url{http://dx.doi.org/XX.XXX/XXXXX}. The reader is encouraged to use this code
to interface with the \texttt{EVEREST} catalog and to customize the de-trending
of targets of interest, particularly for masking transits to remove biases in
the depth due to overfitting. We have implemented each of the PLD models discussed
above (\texttt{rPLD}, \texttt{nPLD}, \texttt{pPLD}) in a general framework that
can be adapted to different missions, including \emph{Kepler} and the upcoming
\emph{TESS}. For more information, refer to the documentation linked 
on the \texttt{github} page.

\acknowledgments{
We would like to thank Benjamin Pope and Tsevi Mazeh
for their useful comments and suggestions.
R.L., R.B., and E.A. acknowledge support from NASA grant
NNX14AK26G and from the NASA Astrobiology Institute’s
Virtual Planetary Laboratory Lead Team, funded through
the NASA Astrobiology Institute under solicitation
NNH12ZDA002C and Cooperative Agreement Number
NNA13AA93A. E.A. acknowledges additional support from
NASA grants NNX13AF20G and NNX13AF62G. E.K.
acknowledges support from an NSF Graduate Student Research
Fellowship. This research used the advanced computational,
storage, and networking infrastructure provided by the Hyak
supercomputer system at the University of Washington.
}

\bibliographystyle{apj}
\bibliography{everest}
\end{document}