\documentclass[]{emulateapj}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath,mathtools}
\usepackage{epsfig}
\usepackage[FIGTOPCAP]{subfigure}
\usepackage{afterpage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{relsize}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{morefloats}
\usepackage{wasysym}

\newcommand{\noop}[1]{}
\newcommand{\note}[1]{{\color{red} #1}}
\newcommand{\cn}{\note{(citation needed)\ }}
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\newcommand{\mearth}{\unit{M_\oplus}}
\newcommand{\rearth}{\unit{R_\oplus}}
\newcommand{\msun}{\unit{M_\odot}}
\newcommand{\lsun}{\unit{L_\odot}}
\newcommand{\mstar}{\unit{M_\star}}
\newcommand{\rj}{\ensuremath{R_\mathrm{J}}}
\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\bavg}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\DeclareMathOperator*{\argmin}{arg\,min}

\shorttitle{EVEREST2.0}
\shortauthors{Luger et al. 2016}

\begin{document}

\title{EVEREST 2.0}
\author{Rodrigo Luger\altaffilmark{1,2}, Eric Agol\altaffilmark{1,2}, Ethan Kruse\altaffilmark{1}, \\
Daniel Foreman-Mackey\altaffilmark{1,3}, Nicholas Saunders\altaffilmark{1}, Rory Barnes\altaffilmark{1,2}}
\altaffiltext{1}{Astronomy Department, University of Washington, Box 351580, Seattle, WA 98195, USA; \href{mailto:rodluger@uw.edu}{rodluger@uw.edu}}
\altaffiltext{2}{Virtual Planetary Laboratory, Seattle, WA 98195, USA}
\altaffiltext{3}{Sagan Fellow}

\begin{abstract}
We present an update to the \texttt{EVEREST} pipeline.
\end{abstract}

\section{Ridge Regression}
\label{sec:l2}
Given a timeseries $\mathbf{y}$ with $N_{dat}$ data points, we wish to find the linear
combination of $N_{reg}$ regressors that best fits the instrumental component of $\mathbf{y}$.
Our linear model is thus
%
\begin{align}
\label{eq:xdotw}
\mathbf{m} = \mathbf{X} \cdot \mathbf{w},
\end{align}
%
where $\mathbf{X}$ is the ($N_{dat} \times N_{reg}$) design matrix constructed from the set
of regressors and $\mathbf{w}$ is the ($N_{reg} \times 1$) vector of weights.
% TODO: Explain ridge regression
The likelihood function for this model is
\begin{align}
\label{eq:like}
\log\mathcal{L} =  &-\frac{1}{2} \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right)^\top
                   \cdot
                   \mathbf{K^{-1}}
                   \cdot
                   \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right) \nonumber\\  
                   %
                   &-\frac{1}{2} \log\left|\mathbf{K}\right| 
                   -\frac{n}{2}\log 2\pi \nonumber\\
                   %
                   &-\frac{1}{2}
                   \mathbf{w}^\top \cdot \mathbf{\Lambda}^{-1} \cdot \mathbf{w}
                   -\frac{1}{2} \log\left|\mathbf{\Lambda}\right|,
\end{align}
%
where $\mathbf{K}$ is the ($N_{dat} \times N_{dat}$) covariance matrix of the data, 
$\mathbf{y}$ is the ($N_{dat} \times 1$) SAP flux, and $\mathbf{\Lambda}$ is the 
($N_{reg} \times N_{reg}$) diagonal regularization matrix,
%
\begin{align}
\label{eq:Lambda}
\Lambda_{m,n} = \lambda_{n}^2\delta_{mn}.
\end{align}
%
Each element $\lambda_n^2$ in $\mathbf{\Lambda}$ is the variance of the 
zero-mean Gaussian prior on the weight of the corresponding column of the design matrix, 
$\mathbf{X}_{*,n}$.

We wish to find the weights $\mathbf{\hat{w}}$ that maximize the posterior probability 
$\mathcal{L}$. Differentiating 
Equation~(\ref{eq:like}) with respect to $\mathbf{w}$, we get
%
\begin{align}
\label{eq:gradlike}
\frac{\mathrm{d}\mathbf{\log\mathcal{L}}}{\mathrm{d}\mathbf{w}} &= 
%
\mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y} \nonumber\\
%
&- \left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right) \cdot \mathbf{w}.
\end{align}
%
By setting this expression equal to zero, we obtain the maximum \emph{a posteriori} prediction 
for the weights,
%
\begin{align}
\label{eq:what}
\mathbf{\hat{w}} = 
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}
\end{align}
with corresponding model
%
\begin{align}
\label{eq:model_slow}
\mathbf{m} = 
%
\mathbf{X} \cdot
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}.
\end{align}
%
If the regularization matrix $\mathbf{\Lambda}$ is known, the de-trended light curve
is simply
\begin{align}
\label{eq:detrended}
\mathbf{y}' = \mathbf{y} - \mathbf{m}.
\end{align}

\section{Cross-validation}
Similarly to Paper I, we solve for $\mathbf{\Lambda}$ by cross-validation. 
% TODO: Details here.
In principle, each of the $\lambda_n$ in $\mathbf{\Lambda}$ could take on a different value, but solving for each one
requires minimizing an $N_{reg}$-dimensional function and is not computationally tractable.
Instead, we simplify the problem by requiring that all regressors of the same order have the
same regularization parameter $\lambda$. Provided we write the third order design matrix in the form
%
\begin{align}
\label{eq:design}
\mathbf{X} = 
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right),
\end{align}
%
where $\mathbf{X_n}$ is the matrix of $n^\mathrm{th}$ order regressors, we may
express the regularization matrix as
%
\begin{align}
\label{eq:Lambda}
\mathbf{\Lambda} = 
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}      &                       & \\
  &                       \mathbf{\Lambda_2}      & \\
  &                       &                       \mathbf{\Lambda_3} \\
\end{array}
\right)
\end{align}
%
where $\mathbf{\Lambda_n} = \lambda_{n}^2\mathbf{I}$ is the 
$n^\mathrm{th}$ order regularization matrix and $\lambda_{n}^2$ is the variance
of the prior on the $n^\mathrm{th}$ order regressors. 

For a typical \emph{K2} star with 30 aperture pixels, $N_{dat} \sim\ $2,000 and
$N_{reg} \sim\ $5,000. Evaluating the matrix inverse in Equation~(\ref{eq:model_slow})
is thus computationally expensive, and becomes prohibitive when performing cross-validation,
since this must be done for every set of $\lambda_{n}$'s. Fortunately, we can greatly reduce 
the number of calculations with some linear algebra. First, we apply the Woodbury matrix 
identity to Equation~(\ref{eq:model_slow}), obtaining
%
\begin{align}
\label{eq:model_woodbury}
\mathbf{m} =      \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top
                  \cdot
                  \left(
                  \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
Next, we note that
%
\begin{align}
\label{eq:separable1}
\mathbf{X} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{\Lambda} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X}^\top &= 
%
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right)
%
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}\hspace*{-8pt}      &                                     & \\[2pt]
  &                                     \mathbf{\Lambda_2}\hspace*{-8pt}      & \\[2pt]
  &                                     &                                     \mathbf{\Lambda_3} \\
\end{array}
\right)
%
\left(
\begin{array}{c}
  \mathbf{X_1^\top} \\[2pt]
  \mathbf{X_2^\top} \\[2pt]
  \mathbf{X_3^\top} \\
\end{array}
\right) \nonumber\\[5pt]
%
&= \lambda_1^2 \mathbf{X_1} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_1} +
   \lambda_2^2 \mathbf{X_2} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_2} +
   \lambda_3^2 \mathbf{X_3} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_3} \nonumber\\[5pt]
%
&= \sum_n \lambda_n^2 \mathbf{X^2_n},
\end{align}
%
where we have defined
%
\begin{align}
\label{eq:x2}
\mathbf{X^2_n} \equiv \mathbf{X_n} \cdot \mathbf{X^\top_n}.
\end{align}
%
We may thus re-write our maximum \emph{a posteriori} model as
%
\begin{align}
\label{eq:model}
\mathbf{m} = \sum_n \lambda_n^2 \mathbf{X^2_n}
                  \cdot
                  \left(
                  \sum_n \lambda_n^2 \mathbf{X^2_n} + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
The matrix that we must invert in Equation~(\ref{eq:model}) has dimensions ($N_{dat} \times N_{dat}$),
while that in Equation~(\ref{eq:model_slow}) is ($N_{reg} \times N_{reg}$). Since
$M \gtrsim 2N$, casting the model in this form can considerably speed up the
computation. In practice, we pre-compute the three matrices $\mathbf{X^2_n}$ at the beginning
of the cross-validation step, so the only time-consuming operation in Equation~(\ref{eq:model})
is the inversion.

Despite these simplifications, solving for the optimal set of regularization parameters $\{ \lambda_1, \lambda_2, \lambda_3 \}$
still requires a costly minimization of the three-dimensional function 
$\sigma_\mathrm{V}(\lambda_1, \lambda_2, \lambda_3)$, where
$\sigma_\mathrm{V}$ is the scatter in the validation set. Our grid along each $\lambda_n$ axis
spans 36 logarithmically-spaced values between $10^{0}$ and $10^{18}$ plus $\lambda_n = 0$, so a full grid search over 
$\sigma_\mathrm{V}$ would require $37^3 =$ 50,653 matrix inversions
in Equation~(\ref{eq:model}).

% TODO: Discuss why using a non-linear minimizer isn't a great idea

In the interest of computational speed, we perform one final simplification. Since we
expect the first order PLD regressors to contain most of the de-trending information,
with each successive PLD order providing a small correction term to the fit, we break
down the minimization problem into three separate one-dimensional problems.
First, we perform cross-validation on the first order PLD model by setting $\lambda_2 = \lambda_3 = 0$
to obtain $\hat{\lambda}_1$. We then perform cross-validation on the second order model
by fixing $\lambda_1$ at this estimate and keeping $\lambda_3 = 0$. Finally, we solve
for $\hat{\lambda}_3$ by fixing the first and second order parameters at their optimum
values:
%
\begin{align}
\hat{\lambda}_1 &= \argmin \sigma_\mathrm{V}(\lambda_1, \lambda_2 = 0,               \lambda_3 = 0) \nonumber\\
\hat{\lambda}_2 &= \argmin \sigma_\mathrm{V}(\lambda_2, \lambda_1 = \hat{\lambda}_1, \lambda_3 = 0) \nonumber\\
\hat{\lambda}_3 &= \argmin \sigma_\mathrm{V}(\lambda_3, \lambda_1 = \hat{\lambda}_1, \lambda_2 = \hat{\lambda}_2).
\end{align}
%
This reduces the number of steps in the grid search to $37\times 3 = 111$, which is much
more computationally tractable. It is important to note that there is no \emph{a priori} reason
that this method should yield the global minimum of $\sigma_\mathrm{V}$; in fact, it very
likely does not. However, since we allow for $\lambda_n = 0$ in our grid search, this approximation
cannot lead to overfitting, as it will always prefer a lower-order PLD model to one
with higher scatter in the validation set. While it may lead to \emph{under}fitting,
in practice we find that it works surprisingly well,
surpassing the de-trending power of the \texttt{EVEREST} model in Paper I (see \S\ref{?}).

%\section{Transit search}
%\label{sec:transit_search}
%Defining
%%
%\begin{align}
%\boldsymbol{\chi} \equiv \sum_n \lambda_n^2 \mathbf{X^2_n},
%\end{align}
%%
%we may write
%%
%\begin{align}
%\mathbf{m} = \left(\boldsymbol{\chi} + \lambda_\tau^2 \boldsymbol{\tau} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\tau^\top} \right)
%             \cdot
%             \left(
%             \boldsymbol{\chi} + \mathbf{K} + \lambda_\tau^2 \boldsymbol{\tau} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\tau^\top}
%             \right)^{-1} 
%             \cdot
%             \mathbf{y}.
%\end{align}
%Applying the Sherman-Morrison formula, the second term in parentheses may be expressed as
%%
%\begin{align}
%\left( \boldsymbol{\chi} + \mathbf{K} \right)^{-1} 
%-
%\frac{
%        \left( \boldsymbol{\chi} + \mathbf{K} \right)^{-1} 
%        \cdot
%        \boldsymbol{\tau} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\tau^\top}
%        \cdot
%        \left( \boldsymbol{\chi} + \mathbf{K} \right)^{-1} 
%}
%{
%        \lambda_\tau^{-2} + \boldsymbol{\tau^\top}
%        \cdot
%        \left( \boldsymbol{\chi} + \mathbf{K} \right)^{-1} 
%        \cdot
%        \boldsymbol{\tau}.
%}
%\end{align}
%Defining
%%
%\begin{align}
%\boldsymbol{\xi} \equiv \left( \boldsymbol{\chi} + \mathbf{K} \right)^{-1},
%\end{align}
%%
%our model becomes
%%
%\begin{align}
%\mathbf{m} = \left(\boldsymbol{\chi} + \lambda_\tau^2 \boldsymbol{\tau} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\tau^\top} \right)
%             \cdot
%             \left(
%             \boldsymbol{\xi} - \frac{\boldsymbol{\xi} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\tau} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\tau^\top} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\xi}}
%                              {\lambda_\tau^{-2} + \boldsymbol{\tau^\top} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\xi} \hspace{-2pt} \cdot \hspace{-2pt} \boldsymbol{\tau}}
%             \right)
%             \cdot
%             \mathbf{y}.
%\end{align}

\section{Injection Tests}
\label{sec:inj}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/injections.pdf}
       \caption{Hello.}
     \label{fig:injections}
  \end{center}
\end{figure}

\clearpage
\bibliographystyle{apj}
\bibliography{everest}
\end{document}