\documentclass[]{emulateapj}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
\usepackage{natbib}
\usepackage{amssymb}
\usepackage{color}
\usepackage{amsmath,mathtools}
\usepackage{epsfig}
\usepackage[FIGTOPCAP]{subfigure}
\usepackage{afterpage}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{relsize}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{morefloats}
\usepackage{wasysym}

\newcommand{\noop}[1]{}
\newcommand{\note}[1]{{\color{red} #1}}
\newcommand{\cn}{\note{(citation needed)\ }}
\newcommand{\unit}[1]{\ensuremath{\, \mathrm{#1}}}
\newcommand{\mearth}{\unit{M_\oplus}}
\newcommand{\rearth}{\unit{R_\oplus}}
\newcommand{\msun}{\unit{M_\odot}}
\newcommand{\lsun}{\unit{L_\odot}}
\newcommand{\mstar}{\unit{M_\star}}
\newcommand{\rj}{\ensuremath{R_\mathrm{J}}}
\newcommand{\avg}[1]{\langle #1 \rangle}
\newcommand{\bavg}[1]{\bigg\langle #1 \bigg\rangle}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\newcommand{\Kp}{\ensuremath{K_\mathrm{p}}}
\DeclareMathOperator*{\argmin}{arg\,min}

\shorttitle{EVEREST2.0}
\shortauthors{Luger et al. 2016}

\begin{document}

\title{EVEREST 2.0}
\author{Rodrigo Luger\altaffilmark{1,2}, Eric Agol\altaffilmark{1,2}, Ethan Kruse\altaffilmark{1}, \\
Daniel Foreman-Mackey\altaffilmark{1,3}, Nicholas Saunders\altaffilmark{1}, Rory Barnes\altaffilmark{1,2}}
\altaffiltext{1}{Astronomy Department, University of Washington, Box 351580, Seattle, WA 98195, USA; \href{mailto:rodluger@uw.edu}{rodluger@uw.edu}}
\altaffiltext{2}{Virtual Planetary Laboratory, Seattle, WA 98195, USA}
\altaffiltext{3}{Sagan Fellow}

\begin{abstract}
We present an update to the \texttt{EVEREST} pipeline.
\end{abstract}

\section{The PLD Model}
\label{sec:model}

\subsection{Ridge Regression}
\label{sec:ridge}
Given a timeseries $\mathbf{y}$ with $N_{dat}$ data points, we wish to find the linear
combination of $N_{reg}$ regressors that best fits the instrumental component of $\mathbf{y}$.
Our linear model is thus
%
\begin{align}
\label{eq:xdotw}
\mathbf{m} = \mathbf{X} \cdot \mathbf{w},
\end{align}
%
where $\mathbf{X}$ is the ($N_{dat} \times N_{reg}$) design matrix constructed from the set
of regressors and $\mathbf{w}$ is the ($N_{reg} \times 1$) vector of weights. If 
$\mathbf{w}$ is known, the de-trended light curve is simply
%
\begin{align}
\label{eq:detrended}
\mathbf{y}' = \mathbf{y} - \mathbf{m}.
\end{align}
%
In Paper I, we obtained $\mathbf{w}$ by maximizing the likelihood function
%
\begin{align}
\label{eq:like}
\log\mathcal{L}_0 =  &-\frac{1}{2} \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right)^\top
                     \cdot
                     \mathbf{K^{-1}}
                     \cdot
                     \left(\mathbf{y} - \mathbf{X} \cdot \mathbf{w}\right) \nonumber\\  
                     %
                     &-\frac{1}{2} \log\left|\mathbf{K}\right| 
                     -\frac{N_{dat}}{2}\log 2\pi,
\end{align}
%
where $\mathbf{K}$ is the ($N_{dat} \times N_{dat}$) covariance matrix of the data and
$\mathbf{y}$ is the ($N_{dat} \times 1$) SAP flux. Since the number of third order PLD regressors can
be quite large (on the order of several thousand for a typical star, which is larger than the number
of data points), the problem is ill-posed, meaning that a unique solution does not exist and 
maximizing $\log\mathcal{L}_0$ is likely to lead to overfitting. We thus constructed $\mathbf{X}$
from the (smaller) set of $N_{pca}$ principal components of the PLD regressors. We chose $N_{pca}$
by performing cross-validation, which aims to maximize the predictive power of the model while
minimizing overfitting.

However, while principal component analysis (PCA) yields a set of components that captures 
the most variance among the PLD vectors, there is no guarantee that the principal components
are the ideal regressors in the PLD problem. Dimensionality reduction techniques such as PCA
inevitably lead to information loss, and so it is worthwhile to consider alternative
regression methods to fully exploit the potential of PLD.

A common regression method for ill-posed problems is regularization, in which a prior is imposed
on the values of the weights $\mathbf{w}$. Since overfitting occurs when $\mathbf{w}$ becomes very
large, regularization recasts the problem by adding a penalty term to the likelihood that 
increases with increasing $|\mathbf{w}|$. While many forms of regularization exist, we focus on
ridge (L2) regression, since it has an analytic solution. Ridge regression involves placing a
Gaussian prior on each of the weights $\mathbf{w}$, so that the posterior likelihood function becomes
%
\begin{align}
\label{eq:like}
\log\mathcal{L} =  \log\mathcal{L}_0
                   -\frac{1}{2}
                   \mathbf{w}^\top \cdot \mathbf{\Lambda}^{-1} \cdot \mathbf{w}
                   -\frac{1}{2} \log\left|\mathbf{\Lambda}\right|,
\end{align}
%
where $\mathbf{\Lambda}$ is the ($N_{reg} \times N_{reg}$) diagonal regularization matrix,
%
\begin{align}
\label{eq:Lambda}
\Lambda_{m,n} = \lambda_{n}^2\delta_{mn}.
\end{align}
%
Each element $\lambda_n^2$ in $\mathbf{\Lambda}$ is the variance of the 
zero-mean Gaussian prior on the weight of the corresponding column of the design matrix, 
$\mathbf{X}_{*,n}$. Provided we choose the $\lambda_{n}$ correctly, this model should 
have a higher predictive power than the PCA model adopted in Paper I.

Given this formulation, our task is to find the weights $\mathbf{\hat{w}}$ that maximize 
the posterior probability 
$\mathcal{L}$. Differentiating 
Equation~(\ref{eq:like}) with respect to $\mathbf{w}$, we get
%
\begin{align}
\label{eq:gradlike}
\frac{\mathrm{d}\mathbf{\log\mathcal{L}}}{\mathrm{d}\mathbf{w}} &= 
%
\mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y} \nonumber\\
%
&- \left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right) \cdot \mathbf{w}.
\end{align}
%
By setting this expression equal to zero, we obtain the maximum \emph{a posteriori} prediction 
for the weights,
%
\begin{align}
\label{eq:what}
\mathbf{\hat{w}} = 
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}
\end{align}
with corresponding model
%
\begin{align}
\label{eq:model_slow}
\mathbf{m} = 
%
\mathbf{X} \cdot
%
\left( \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{X} + \mathbf{\Lambda}^{-1} \right)^{-1}
%
\cdot \mathbf{X}^\top \cdot \mathbf{K}^{-1} \cdot \mathbf{y}.
\end{align}

\subsection{Cross-validation}
\label{sec:crossval}
Similarly to Paper I, we solve for $\mathbf{\Lambda}$ by cross-validation. For each value
of $\mathbf{\Lambda}$, the model is trained on one part of the light curve (the training set)
and used to de-trend the other part of the light curve (the validation set). The value of 
$\mathbf{\Lambda}$ that results in the minimum scatter in the validation set is then chosen for
the final de-trending step. 

In principle, each of the $\lambda_n$ in $\mathbf{\Lambda}$ could take on a different value, 
but solving for each one requires minimizing an $N_{reg}$-dimensional function and is not computationally tractable.
Instead, we simplify the problem by requiring that all regressors of the same order have the
same regularization parameter $\lambda$. Provided we write the third order design matrix in the form
%
\begin{align}
\label{eq:design}
\mathbf{X} = 
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right),
\end{align}
%
where $\mathbf{X_n}$ is the matrix of $n^\mathrm{th}$ order regressors, we may
express the regularization matrix as
%
\begin{align}
\label{eq:Lambda}
\mathbf{\Lambda} = 
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}      &                       & \\
  &                       \mathbf{\Lambda_2}      & \\
  &                       &                       \mathbf{\Lambda_3} \\
\end{array}
\right)
\end{align}
%
where $\mathbf{\Lambda_n} = \lambda_{n}^2\mathbf{I}$ is the 
$n^\mathrm{th}$ order regularization matrix and $\lambda_{n}^2$ is the variance
of the prior on the $n^\mathrm{th}$ order regressors. 

A typical $K2$ star with 30 aperture pixels has $N_{reg} \sim\ $5,000 regressors and
$N_{dat} \sim\ $500 data points in each cross-validation light curve segment 
(see \S\ref{sec:implementation}). Evaluating the matrix inverse in Equation~(\ref{eq:model_slow})
is thus computationally expensive, and becomes prohibitive during cross-validation,
since this must be done for every set of $\lambda_{n}$'s. Fortunately, we can reduce 
the number of calculations with some linear algebra. First, we apply the Woodbury matrix 
identity to Equation~(\ref{eq:model_slow}), obtaining
%
\begin{align}
\label{eq:model_woodbury}
\mathbf{m} =      \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top
                  \cdot
                  \left(
                  \mathbf{X} \cdot \mathbf{\Lambda} \cdot \mathbf{X}^\top + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
Next, we note that
%
\begin{align}
\label{eq:separable1}
\mathbf{X} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{\Lambda} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X}^\top &= 
%
\left(
\begin{array}{ccc}
  \mathbf{X_1} & \mathbf{X_2} & \mathbf{X_3}
\end{array}
\right)
%
\left(
\begin{array}{ccc}
  \mathbf{\Lambda_1}\hspace*{-8pt}      &                                     & \\[2pt]
  &                                     \mathbf{\Lambda_2}\hspace*{-8pt}      & \\[2pt]
  &                                     &                                     \mathbf{\Lambda_3} \\
\end{array}
\right)
%
\left(
\begin{array}{c}
  \mathbf{X_1^\top} \\[2pt]
  \mathbf{X_2^\top} \\[2pt]
  \mathbf{X_3^\top} \\
\end{array}
\right) \nonumber\\[5pt]
%
&= \lambda_1^2 \mathbf{X_1} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_1} +
   \lambda_2^2 \mathbf{X_2} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_2} +
   \lambda_3^2 \mathbf{X_3} \hspace{-2pt} \cdot \hspace{-2pt} \mathbf{X^\top_3} \nonumber\\[5pt]
%
&= \sum_n \lambda_n^2 \mathbf{X^2_n},
\end{align}
%
where we have defined
%
\begin{align}
\label{eq:x2}
\mathbf{X^2_n} \equiv \mathbf{X_n} \cdot \mathbf{X^\top_n}.
\end{align}
%
We may thus re-write our maximum \emph{a posteriori} model as
%
\begin{align}
\label{eq:model}
\mathbf{m} = \sum_n \lambda_n^2 \mathbf{X^2_n}
                  \cdot
                  \left(
                  \sum_n \lambda_n^2 \mathbf{X^2_n} + \mathbf{K}
                  \right)^{-1} 
                  \cdot
                  \mathbf{y}.
\end{align}
%
The matrix that we must invert in Equation~(\ref{eq:model}) has dimensions ($N_{dat} \times N_{dat}$),
while that in Equation~(\ref{eq:model_slow}) is ($N_{reg} \times N_{reg}$). Since
$N_{reg} \sim 10N_{dat}$, casting the model in this form can greatly speed up the
computation. In practice, we pre-compute the three matrices $\mathbf{X^2_n}$ at the beginning
of the cross-validation step, so the only time-consuming operation in Equation~(\ref{eq:model})
is the inversion.

\subsection{Neighboring Stars}
\label{sec:neighboring}
The principal difference between PLD and other methods used to correct $K2$ light curves
\citep[e.g.,][]{Vanderburg14,ForemanMackey15,Aigrain16} is in the origin of the information
used in the de-trending. PLD uses \emph{local} information, present in a given star's pixels,
to de-trend that star's light curve; alternative methods almost exclusively use \emph{global} information,
derived from the behaviour of many stars across the detector, to accomplish the same thing. 
It is perhaps surprising that PLD works so well, given that the signal-to-noise ratio of the regressors
of the latter methods is \emph{necessarily} higher, since they are derived from hundreds to
thousands of stars and thus contain information about the spacecraft motion at high fidelity.
The fact that PLD outperforms other pipelines --- and in particular its ability to correct
data collected during thruster firings --- suggests that local modes contribute significantly
to the instrumental noise in $K2$ light curves. Local noise is due to intra- and inter-pixel
variability and local pixel response function (PRF) behavior and is specific to the properties of 
the detector at the particular location of the target, while global noise is due solely to 
the motion of the spacecraft and is shared among all targets. Traditional methods can only
correct the latter (but do so extremely well). PLD excels at correcting the former. While
PLD certainly removes the noise introduced by the spacecraft motion (i.e., global noise), its
regressors are noisy, particularly for dimmer targets. This suggests that a combination of
both approaches that explicitly de-trends both local and global noise could substantially improve PLD,
especially for faint stars.

A simple way of implementing this would be to include the two \emph{Kepler} motion vectors (the
\texttt{POS\_CORR1} and \texttt{POS\_CORR2} fields in the target pixel files), as well as
higher order products of these, in the regression, assigning each their own L2 priors ($\mathbf{\Lambda}$).
While this approach works, in practice we obtain higher de-trending power by instead including the PLD
vectors of bright neighboring targets in the regression; we dub this method \texttt{nPLD}, for \emph{n}eighboring
\emph{PLD}. Our third-order design matrix (Equation~\ref{eq:design}) is now
%
\begin{align}
\label{eq:design_nPLD}
\mathbf{X} = 
\left(
\begin{array}{cccccc}
  \mathbf{X_1} & \mathbf{X_1'} & \mathbf{X_2} & \mathbf{X_2'} & \mathbf{X_3} & \mathbf{X_3'}
\end{array}
\right),
\end{align}
%
where $\mathbf{X_n'}$ is the design matrix constructed from the $n^{th}$ order PLD vectors of all the 
neighboring targets. For computational speed, we still solve for a single prior amplitude $\lambda_n$ 
for each PLD order, but in principle one could assign different priors to the neighboring vectors. We
discuss the implementation of \texttt{nPLD} in \S\ref{sec:implementation}.

\section{Implementation}
\label{sec:implementation}

\subsection{Outlier clipping}
\label{sec:impl_outliers}
We perform iterative sigma clipping.

\subsection{GP Optimization}
\label{sec:impl_gp}
We optimize the GP. To prevent too much power in PLD, we initialize GP at large amplitude.

\subsection{Breakpoints}
\label{sec:impl_breakpoints}
We split the light curves up.

\subsection{Neighboring Targets}
\label{sec:impl_neighboring}
We select neighboring targets.

\subsection{Cross-validation}
\label{sec:impl_crossval}
Solving for the optimal set of regularization parameters 
$\{ \lambda_1, \lambda_2, \lambda_3 \}$
requires a costly minimization of the three-dimensional function 
$\sigma_\mathrm{v}(\lambda_1, \lambda_2, \lambda_3)$, where
$\sigma_\mathrm{v}$ is the scatter in the validation set.
%
In the interest of computational speed, we perform a simplification. Since we
expect the first order PLD regressors to contain most of the de-trending information,
with each successive PLD order providing a small correction term to the fit, we break
down the minimization problem into three separate one-dimensional problems.
First, we perform cross-validation on the first order PLD model by setting $\lambda_2 = \lambda_3 = 0$
to obtain $\hat{\lambda}_1$. We then perform cross-validation on the second order model
by fixing $\lambda_1$ at this estimate and keeping $\lambda_3 = 0$. Finally, we solve
for $\hat{\lambda}_3$ by fixing the first and second order parameters at their optimum
values:
%
\begin{align}
\hat{\lambda}_1 &= \argmin \sigma_\mathrm{v}(\lambda_1)\bigg|_{\lambda_2 = 0,\               \lambda_3 = 0} \nonumber\\
\hat{\lambda}_2 &= \argmin \sigma_\mathrm{v}(\lambda_2)\bigg|_{\lambda_1 = \hat{\lambda}_1,\ \lambda_3 = 0} \nonumber\\
\hat{\lambda}_3 &= \argmin \sigma_\mathrm{v}(\lambda_3)\bigg|_{\lambda_1 = \hat{\lambda}_1,\ \lambda_2 = \hat{\lambda}_2}
\end{align}
%
%This reduces the number of steps in the grid search to $37\times 3 = 111$, which is much
%more computationally tractable. 
It is important to note that there is no \emph{a priori} reason
that this method should yield the global minimum of $\sigma_\mathrm{v}$; in fact, it very
likely does not. However, since we allow for $\lambda_n = 0$ in our grid search, this approximation
cannot lead to overfitting, as it will always prefer a lower-order PLD model to one
with higher scatter in the validation set. While it may lead to \emph{under}fitting,
in practice we find that it works surprisingly well,
surpassing the de-trending power of the \texttt{EVEREST} model in Paper I (see \S\ref{?}).

\section{Results}
\label{sec:results}

\subsection{Ridge Regression}
\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/ridge_reg.pdf}
       \caption{6-hr CDPP comparison between de-trending with ridge (L2) regression (this paper) and
                de-trending with PCA (Paper I) for a sample of 2,700 randomly selected campaign 6
                stars. Plotted is the star-by-star difference in the CDPP values for each pipeline,
                normalized to the PCA CDPP (blue dots); stars with negative values have lower CDPP 
                when de-trended with ridge regression. The black line is the median CDPP difference
                in 0.5 magnitude-wide bins. Ridge regression leads to a small CDPP improvement of 
                ${\sim}1$ to 5\%.}
     \label{fig:ridge_reg}
  \end{center}
\end{figure}

\subsection{Neighboring PLD}
\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/nPLD.pdf}
       \caption{6-hr CDPP comparison between PLD de-trending with ridge regression + neighboring targets 
                (this paper) and standard PLD de-trending (Paper I) for the same sample of stars as in Figure~\ref{fig:ridge_reg}.
                Each target was de-trended with its own PLD vectors plus those of ten random bright stars
                on the same module. This method leads to a robust CDPP improvement of ${\sim}10$\% for bright ($\Kp \lesssim 13$) stars
                and ${\sim}20$\% for fainter stars.}
     \label{fig:nPLD}
  \end{center}
\end{figure}

\section{Saturated Stars}
\label{sec:saturated}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/saturated_star.pdf}
       \caption{EPIC 202063160, a saturated \Kp = 9.2 campaign 0 eclipsing
       binary. Shown is a portion of the raw light curve (top), the light curve ``de-trended''
       with \texttt{EVEREST 1.0} (center), and the light curve de-trended with \texttt{EVEREST 2.0}
       (bottom). The pixel image is shown at the right on a linear scale, with the adopted aperture
       contour indicated in red. The three columns highlighted in red contain saturated
       pixels. Despite a great improvement in the CDPP, \texttt{EVEREST 1.0} leads to severe
       overfitting, causing the eclipses to all but disappear. This is due to the fact that
       saturated pixels contain nearly no astrophysical information, as the signal
       overflows into the pixels at the top and bottom of the saturated columns; this 
       invalidates the basic assumption of PLD that all pixels contain the same amount
       of astrophysical information. To correct for this, we collapse each saturated
       column into a single pixel. Since the flux is approximately conserved along the bleed
       trail, this method ensures that astrophysical information is constant across the aperture.
       While this leads to the loss of some of the instrumental components used by PLD,
       in practice we find that the unsaturated pixels still contain enough
       information to properly de-trend the target.}
     \label{fig:saturated_star}
  \end{center}
\end{figure}

\section{Comparison to Other Pipelines}
\label{sec:comparison}

\subsection{CDPP}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_everest1_all.pdf}
       \caption{CDPP comparison between \texttt{EVEREST 2.0} and \texttt{EVEREST 1.0} for all stars in campaigns 0--8.
       As before, individual stars are plotted as blue points and the median CDPP is indicated by a black line; note
       the ${\sim}10-20$\% improvement over the previous version of the pipeline. Saturated
       stars are plotted as red points, with their median CDPP indicated by a dashed red line. The apparently
       better performance of \texttt{EVEREST 1.0} for these stars is spurious, since traditional PLD typically
       leads to strong overfitting of saturated stars (see Figure~\ref{fig:saturated_star}).}
     \label{fig:cdpp_everest1_all}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sff_all.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_everest1_all}, but showing a comparison between \texttt{EVEREST 2.0} and \texttt{K2SFF}.
       \texttt{EVEREST 2.0} outperforms \texttt{K2SFF} at all magnitudes, including $\Kp \lesssim 11$, for which stars are saturated.}
     \label{fig:cdpp_k2sff_all}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sc_all.pdf}
       \caption{Similar to the previous figure, but showing a comparison between \texttt{EVEREST 2.0} and \texttt{K2SC}.
       \texttt{EVEREST 2.0} light curves have lower average CDPP at all magnitudes except around $\Kp \approx 9$, for which
       the precision is comparable.}
     \label{fig:cdpp_k2sc_all}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_everest1.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_everest1_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{EVEREST 1.0} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_everest1}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sff.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_k2sff_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{K2SFF} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_k2sff}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/cdpp_k2sc.pdf}
       \caption{Similar to Figure~\ref{fig:cdpp_k2sc_all}, but showing a CDPP comparison between 
       \texttt{EVEREST 2.0} and \texttt{K2SC} for each of the first 8 $K2$ campaigns.}
     \label{fig:cdpp_k2sc}
  \end{center}
\end{figure}

\subsection{Comparison to Kepler}

\begin{figure*}[t]
  \begin{center}
      \leavevmode
      \includegraphics[width=\textwidth]{figures/cdpp_kepler.pdf}
       \caption{6-hr photometric precision as a function of \emph{Kepler} magnitude $\Kp$ for all 
       stars observed by \emph{Kepler} (yellow dots) and for all $K2$ targets in Campaigns 0-8
       de-trended with \texttt{EVEREST} (blue). The median in 0.5 magnitude-wide bins is indicated
       by yellow circles for \emph{Kepler} and by blue circles for \texttt{EVEREST}. For campaigns
       1, 5, and 6, \texttt{EVEREST} recovers the raw \emph{Kepler} photometric precision down to
       at least $\Kp = 15$; for campaigns 3, 4, and 8, \texttt{EVEREST} recovers the \emph{Kepler} 
       precision down to $\Kp = 14$. Campaigns 0 and 2 have a larger fraction of (variable) giant
       stars, leading to a higher average CDPP, while campaign 7 raw light curves have significantly
       worse precision due to a change in the orientation of the spacecraft and excess jitter.
       }
     \label{fig:cdpp_kepler}
  \end{center}
\end{figure*}

\subsection{Outliers}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/outliers.pdf}
       \caption{Histograms showing the number of non-outlier data points per campaign for each
        of four pipelines: \texttt{K2SFF} (gray), \texttt{K2SC} (orange), \texttt{EVEREST 1.0}
        (red), and \texttt{EVEREST 2.0} (blue). To compute these, we remove all cadences 
        with flagged \texttt{QUALITY} bits (excepting thruster fires) from all light curves, then
        smooth each light curve with a second order, 2-day Savitsky-Golay filter and perform iterative 
        sigma clipping at 5-$\sigma$ to remove the outliers. The number of remaining cadences is
        for each light curve is then used to plot the histograms. Both versions of \texttt{EVEREST}
        have more usable data points per campaign than the other pipelines. On average, 
        \texttt{EVEREST} light curves have ${\sim}200-300$ more non-outlier data points than \texttt{K2SFF} 
        and ${\sim}100$ more than \texttt{K2SC}.}
     \label{fig:outliers}
  \end{center}
\end{figure}

\section{Injection Tests}
\label{sec:inj}

\begin{figure}[ht]
  \begin{center}
      \includegraphics[width=0.47\textwidth]{figures/injections.pdf}
       \caption{Transit injection/recovery statistics based on 2,700 randomly selected stars from campaign 6.
       Each panel shows histograms of the number of transits recovered with a certain depth 
       ratio $D/D_0$ (recovered depth divided by true depth). Blue histograms correspond to the actual
       injection and recovery process, in which transits are injected into the raw light curves at the pixel level
       and recovered after de-trending with \texttt{EVEREST}; red histograms correspond to control runs in which the transits
       were injected into the \emph{de-trended} data. The values 
       to the left and right of each histogram are the median $D/D_0$ for our pipeline and for the 
       control run, respectively. The smaller values at the top indicate the fraction of transits recovered 
       with depths lower and higher than the bounds of the plots. Finally, the two columns distinguish between 
       runs in which the transits were explicitly masked prior to de-trending (left) and runs in which they were not (right), 
       while the three rows correspond to different injected depths: $10^{−2}$, $10^{−3}$, and $10^{−4}$. \texttt{EVEREST} 
       preserves transit depths if the transits are properly masked; otherwise, a ${\sim}10\%$ bias toward smaller depths is 
       introduced for transits with low signal-to-noise.}
     \label{fig:injections}
  \end{center}
\end{figure}

\section{CBVs}
\label{sec:cbvs}

\begin{figure}[h]
	\centering   
	\subfigure[CBV \#1]{
		\label{fig:cbv1}
		\includegraphics[width=0.2\textwidth]{figures/cbv1.pdf}
	}
	\subfigure[CBV \#2]{
		\label{fig:cbv2}
		\includegraphics[width=0.2\textwidth]{figures/cbv2.pdf}
	}
	\caption{Campaign 2 Co-trending Basis Vectors (CBVs). We apply \texttt{SysRem} to all 
	         de-trended light curves in each of the 19 functioning modules on the \emph{Kepler}
	         CCD. Plotted are the \textbf{(a)} first and \textbf{(b)} second CBVs recovered
	         for each module. Since campaign 2 light curves are de-trended in two separate
	         segments, we apply \texttt{SysRem} to each segment separately (blue and red
	         curves). The first set of CBVs contain primarily linear trends with the hook-like
	         thermal features at the beginning of the campaign; the second set of CBVs are
	         dominated by quadratic trends. We correct all campaign 2 light curves by simple
	         linear regression with the first two CBVs, with the exception of those on module
	         20, whose second CBV signal displays higher frequency oscillations.
	         This module contains fewer stars than the other modules, leading to CBVs that contain
	         a higher amount of astrophysical information. For this module and for modules in 
	         other campaigns whose second CBV is not predominantly quadratic, we correct the
	         light curves with a single CBV. \label{fig:cbv}}
\end{figure}

\bibliographystyle{apj}
\bibliography{everest}
\end{document}